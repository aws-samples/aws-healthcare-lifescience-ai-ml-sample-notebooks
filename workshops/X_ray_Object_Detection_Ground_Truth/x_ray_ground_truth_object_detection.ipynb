{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Ground Truth Labeled Data to Train an Object Detection Model in Chest Xrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we demonstrate how to build a machine learning model to detect the trachea of a patient in an x-ray image using Amazon SageMaker.\n",
    "We will be using 1099 NIH Chest X-ray images sampled from [this](https://www.kaggle.com/nih-chest-xrays/data) repository. While the images are originally from that source, we leveraged SageMaker Ground Truth to create bounding boxes around the trachea of the patient. We will thus be using **both** the raw images and also the manifest file where labellers labeled the trachea of the patient. \n",
    "An example of a labeled image is:\n",
    "\n",
    "![image.png](chest_image.png)\n",
    "\n",
    "This process could potentially be used as a template for detecting other objects as well within xrays; however, we focus only on detecting the trachea of the patient, if it is present.\n",
    "This notebook contains instructions to use the GroundTruth manifest file to understand the labeled data, train, build and deploy the model as an end point in SageMaker. This notebook is created on a \"ml.t3.medium\" instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives:\n",
    "\n",
    "This workshop covers a basic introduction to SageMaker Ground truth, understanding the labeled images, split the dataset for training and validation, building, training, deploying and testing an object detection model in SageMaker. Here are the steps:\n",
    "\n",
    "1. Perform basic preprocessing of images using the ground truth manifest file\n",
    "2. Visualize the labeled images for data analysis and understanding\n",
    "3. Process SageMaker Ground Truth [manifest files](https://docs.aws.amazon.com/lookout-for-vision/latest/developer-guide/create-dataset-ground-truth.html).\n",
    "4. Build, train, deploy and test the SageMaker built-in object detection model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leverage image dataset located in an S3 bucket\n",
    "\n",
    "### Public Dataset Used:\n",
    "\n",
    "Chest X-ray images are stored in publicly accessible S3 bucket. Below lines of code will download the image data from public S3 bucket to user's S3 bucket.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import json\n",
    "import numpy as np\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "default_bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data source\n",
    "DATA_SOURCE='s3://aws-hcls-ml/public_assets_support_materials/x_ray_object_detection_data/'\n",
    "\n",
    "BUCKET = default_bucket #optional: Change to your bucket\n",
    "PREFIX='x_ray_image_data' #optional: Change to your directory/prefix\n",
    "\n",
    "IMAGE_DATA_S3=f's3://{BUCKET}/{PREFIX}/' #location of image data in s3\n",
    "\n",
    "!aws s3 cp $DATA_SOURCE $IMAGE_DATA_S3 --recursive --quiet\n",
    "!echo \"Image data copied to \"$IMAGE_DATA_S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to SageMaker Ground Truth\n",
    "\n",
    "Amazon SageMaker enables you to identify raw data, such as images, text files, and videos; add informative labels; and generate labeled synthetic data to create high-quality training datasets for your machine learning (ML) models. SageMaker offers two options, Amazon SageMaker Ground Truth Plus and Amazon SageMaker Ground Truth, which provide you with the flexibility to use an expert workforce to create and manage data labeling workflows on your behalf or manage your own data labeling workflows.\n",
    "\n",
    "If you want the flexibility to build and manage your own data labeling workflows and workforce, you can use SageMaker Ground Truth. SageMaker Ground Truth is a data labeling service that makes it easy to label data and gives you the option to use human annotators through Amazon Mechanical Turk, third-party vendors, or your own private workforce.\n",
    "\n",
    "You can also generate labeled synthetic data without manually collecting or labeling real-world data. SageMaker Ground Truth can generate hundreds of thousands of automatically labeled synthetic images on your behalf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manifest file\n",
    "\n",
    "A manifest file contains information about the images and image labels that you can use to train and test a model. Each line in an input manifest file is an entry containing an object, or a reference to an object, to label. An entry can also contain labels from previous jobs and for some task types, additional information. In the example below, the pixel for the start of the box that contains the trachea is at pixel `[420,15]` and the image has a height of 108 and a width of 143.\n",
    "\n",
    "Here is a sample record from manifest file:\n",
    "```\n",
    "{\n",
    "  \"source-ref\": \"s3://BUCKET/PREFIX/image_data/00000001_000.png\",\n",
    "  \"xray-labeling-job-clone-clone-full-clone\": {\n",
    "    \"annotations\": [\n",
    "      {\n",
    "        \"class_id\": 0,\n",
    "        \"width\": 143,\n",
    "        \"top\": 15,\n",
    "        \"height\": 108,\n",
    "        \"left\": 420\n",
    "      }\n",
    "    ],\n",
    "    \"image_size\": [\n",
    "      {\n",
    "        \"width\": 1024,\n",
    "        \"depth\": 3,\n",
    "        \"height\": 1024\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  \"xray-labeling-job-clone-clone-full-clone-metadata\": {\n",
    "    \"class-map\": {\n",
    "      \"0\": \"Trachea\"\n",
    "    },\n",
    "    \"objects\": [\n",
    "      {\n",
    "        \"confidence\": 0\n",
    "      }\n",
    "    ],\n",
    "    \"job-name\": \"labeling-job/xray-labeling-job-clone-clone-full-clone\",\n",
    "    \"human-annotated\": \"yes\",\n",
    "    \"creation-date\": \"2020-07-22T15:04:38.513000\",\n",
    "    \"type\": \"groundtruth/object-detection\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "In our workshop, we will be using a template manifest file from the Ground Truth labeling job that was used to label 1099 images from the source dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new manifest file from the template\n",
    "\n",
    "The below lines of code will replace the bucket and prefix values and creates a new manifest file. This step replaces the BUCKET and PREFIX strings in the template file with the one you will actually used based on your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data #make a data directory if it does not exist\n",
    "with open('template.manifest', 'r') as template_file:\n",
    "    output = [json.loads(line.strip().replace('BUCKET',BUCKET).replace('PREFIX',PREFIX)) for line in template_file.readlines()]\n",
    "\n",
    "with open('data/output_manifest_clean.manifest','w') as output_file:\n",
    "    for i in output:\n",
    "        json.dump(i, output_file)\n",
    "        output_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the labeled images\n",
    "\n",
    "1. Add bounding box to the images that corresponds to throat/trachea labels\n",
    "2. Create labeled and non-labeled image lists for model training and validation. \n",
    "\n",
    "There must be <b><u>851</u></b> labeled images and <b><u>248</u></b> non-labeled images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ground_truth_utils import extract_image_data, WorkerBoundingBox\n",
    "\n",
    "output_images_with_bounding_box = extract_image_data(output)\n",
    "\n",
    "# Iterate through the json files, creating bounding box objects.\n",
    "output_with_answers = []  # only include images with the answers in them\n",
    "output_images_with_answers = []\n",
    "output_with_no_answers = []\n",
    "output_images_with_no_answers = []\n",
    "\n",
    "# Find the job name the manifest corresponds to.\n",
    "keys = list(output[0].keys())\n",
    "metakey = keys[np.where([(\"-metadata\" in k) for k in keys])[0][0]]\n",
    "jobname = metakey[:-9]\n",
    "\n",
    "for i in range(0, len(output)):\n",
    "    try:\n",
    "        # images with class_id have answers in them\n",
    "        x = output[i][jobname][\"annotations\"][0][\"class_id\"]\n",
    "        output_with_answers.append(output[i])\n",
    "        output_images_with_answers.append(output_images_with_bounding_box[i])\n",
    "    except:\n",
    "        output_with_no_answers.append(output[i])\n",
    "        output_images_with_no_answers.append(output_images_with_bounding_box[i])\n",
    "        pass\n",
    "\n",
    "# add the box to the image\n",
    "for i in range(0, len(output_with_answers)):\n",
    "    the_output = output_with_answers[i]\n",
    "    the_image = output_images_with_answers[i]\n",
    "    answers = the_output[jobname][\"annotations\"]\n",
    "    box = WorkerBoundingBox(image_id=i, boxdata=answers[0], worker_id=\"anon-worker\")\n",
    "    box.image = the_image\n",
    "    the_image.worker_boxes.append(box)\n",
    "\n",
    "print(\n",
    "    f\"Number of images with labeled trachea/throat: {len(output_images_with_answers)}\"\n",
    ")\n",
    "print(f\"Number of images without labeled trachea/throat: {len(output_with_no_answers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect image labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download 5 random images from the labeled images list and store them in data directory. Loop through the sample images and plot them for visualization. The visualization shows the label with bounding box around the throat and trachea. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SHOW = 5\n",
    "image_subset = np.random.choice(output_images_with_answers, N_SHOW, replace=False)\n",
    "\n",
    "# Download image data\n",
    "for img in image_subset:\n",
    "    target_fname = f\"data/{img.uri.split('/')[-1]}\"\n",
    "    !aws s3 cp {img.uri} {target_fname} --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Find human and auto-labeled images in the subset.\n",
    "human_labeled_subset = [img for img in image_subset if img.human]\n",
    "\n",
    "# Show examples of each\n",
    "for img in human_labeled_subset:\n",
    "    fig, axes = plt.subplots(facecolor='white', dpi=100)\n",
    "    fig.suptitle('Human-labeled examples', fontsize=14)\n",
    "    img.download(\"data\")\n",
    "    img.plot_consolidated_bbs(axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Train, Validation and Test Datasets\n",
    "\n",
    "Shuffle the records read from the manifest file. Split the data into training, validation and holdout sets. For each set create corresponding manifest file and upload them to S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=output_with_answers\n",
    "EXP_NAME= 'sm-object-detection'\n",
    "\n",
    "# Shuffle output in place.\n",
    "np.random.shuffle(output)\n",
    "    \n",
    "dataset_size = len(output)\n",
    "train_test_split_index = round(dataset_size*0.9)\n",
    "\n",
    "train_data = output[:train_test_split_index]\n",
    "test_data = output[train_test_split_index:]\n",
    "\n",
    "train_test_split_index_2 = round(len(test_data)*0.5)\n",
    "validation_data=test_data[:train_test_split_index_2]\n",
    "hold_out=test_data[train_test_split_index_2:]\n",
    "                                 \n",
    "\n",
    "num_training_samples = 0\n",
    "attribute_names = []\n",
    "with open('data/train.manifest', 'w') as f:\n",
    "    for line in train_data:\n",
    "        f.write(json.dumps(line))\n",
    "        f.write('\\n')\n",
    "        num_training_samples += 1\n",
    "        attribute_names = [attrib for attrib in line.keys() if 'meta' not in attrib]\n",
    "    \n",
    "with open('data/validation.manifest', 'w') as f:\n",
    "    for line in validation_data:\n",
    "        f.write(json.dumps(line))\n",
    "        f.write('\\n')\n",
    "        \n",
    "with open('data/hold_out.manifest', 'w') as f:\n",
    "    for line in hold_out:\n",
    "        f.write(json.dumps(line))\n",
    "        f.write('\\n')\n",
    "        \n",
    "print(f'Training Data Set Size: {len(train_data)}')\n",
    "print(f'Validatation Data Set Size: {len(validation_data)}')\n",
    "print(f'Hold Out Data Set Size: {len(hold_out)}')\n",
    "\n",
    "!aws s3 cp data/train.manifest s3://{BUCKET}/{PREFIX}/{EXP_NAME}/train.manifest --quiet\n",
    "!aws s3 cp data/validation.manifest s3://{BUCKET}/{PREFIX}/{EXP_NAME}/validation.manifest --quiet\n",
    "!aws s3 cp data/hold_out.manifest s3://{BUCKET}/{PREFIX}/{EXP_NAME}/hold_out.manifest --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker training job setup\n",
    "\n",
    "In this section, the SageMaker built-in object detection algorithm is used to train the model with it's corresponding datasets (training and validation datasets) as input channels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "s3_resource = boto3.resource(\"s3\")\n",
    "\n",
    "# Using the builtin object detection algorithm in SageMaker\n",
    "training_image = sagemaker.image_uris.retrieve(\n",
    "    \"object-detection\", boto3.Session().region_name\n",
    ")\n",
    "augmented_manifest_filename_train = \"train.manifest\"\n",
    "augmented_manifest_filename_validation = \"validation.manifest\"\n",
    "\n",
    "# Defines paths for use in the training job request.\n",
    "s3_output_path = f\"s3://{BUCKET}/{PREFIX}/{EXP_NAME}/output\"\n",
    "s3_train_data_path = (\n",
    "    f\"s3://{BUCKET}/{PREFIX}/{EXP_NAME}/{augmented_manifest_filename_train}\"\n",
    ")\n",
    "s3_validation_data_path = (\n",
    "    f\"s3://{BUCKET}/{PREFIX}/{EXP_NAME}/{augmented_manifest_filename_validation}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Manifest location for training data: {s3_train_data_path}\")\n",
    "print(f\"Manifest location for validation data: {s3_validation_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify SageMaker training job attributes\n",
    "\n",
    "In this step, we will be set all the attributes required for model training. The attributes include a unique job name, role, output path, hyperparameters for the model, training and validation paths, number of training samples and instance details. \n",
    "\n",
    "Please note that this model can only be trained using an GPU instance. Refer: https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection.html\n",
    "\n",
    "For object detection, SageMaker currently supports following GPU instances for training: \n",
    "ml.p2.xlarge, ml.p2.8xlarge, ml.p2.16xlarge, ml.p3.2xlarge, ml.p3.8xlarge and ml.p3.16xlarge. \n",
    "\n",
    "We will be using ml.p2.xlarge to train this model. This instance takes about an hour to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from time import gmtime, strftime\n",
    "\n",
    "# Create unique job name\n",
    "job_name_prefix = EXP_NAME\n",
    "timestamp = time.strftime(\"-%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "model_job_name = job_name_prefix + timestamp\n",
    "\n",
    "training_params = {\n",
    "    \"AlgorithmSpecification\": {\n",
    "        # NB. This is one of the named constants defined in the first cell.\n",
    "        \"TrainingImage\": training_image,\n",
    "        \"TrainingInputMode\": \"Pipe\",\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"OutputDataConfig\": {\"S3OutputPath\": s3_output_path},\n",
    "    \"ResourceConfig\": {\n",
    "        \"InstanceCount\": 1,\n",
    "        \"InstanceType\": \"ml.p2.xlarge\",  # Use a GPU backed instance\n",
    "        \"VolumeSizeInGB\": 50,\n",
    "    },\n",
    "    \"TrainingJobName\": model_job_name,\n",
    "    \"HyperParameters\": {  # NB. These hyperparameters are at the user's discretion and are beyond the scope of this demo.\n",
    "        \"base_network\": \"resnet-50\",\n",
    "        \"use_pretrained_model\": \"1\",\n",
    "        \"num_classes\": \"1\",\n",
    "        \"mini_batch_size\": \"10\",\n",
    "        \"epochs\": \"30\",\n",
    "        \"learning_rate\": \"0.001\",\n",
    "        \"lr_scheduler_step\": \"\",\n",
    "        \"lr_scheduler_factor\": \"0.1\",\n",
    "        \"optimizer\": \"sgd\",\n",
    "        \"momentum\": \"0.9\",\n",
    "        \"weight_decay\": \"0.0005\",\n",
    "        \"overlap_threshold\": \"0.5\",\n",
    "        \"nms_threshold\": \"0.45\",\n",
    "        \"image_shape\": \"300\",\n",
    "        \"label_width\": \"350\",\n",
    "        \"num_training_samples\": str(num_training_samples),\n",
    "    },\n",
    "    \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 86400},\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"AugmentedManifestFile\",  # NB. Augmented Manifest\n",
    "                    \"S3Uri\": s3_train_data_path,\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                    # NB. This must correspond to the JSON field names in your augmented manifest.\n",
    "                    \"AttributeNames\": attribute_names,\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"application/x-recordio\",\n",
    "            \"RecordWrapperType\": \"RecordIO\",\n",
    "            \"CompressionType\": \"None\",\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"AugmentedManifestFile\",  # NB. Augmented Manifest\n",
    "                    \"S3Uri\": s3_validation_data_path,\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                    # NB. This must correspond to the JSON field names in your augmented manifest.\n",
    "                    \"AttributeNames\": attribute_names,\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"application/x-recordio\",\n",
    "            \"RecordWrapperType\": \"RecordIO\",\n",
    "            \"CompressionType\": \"None\",\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "print(\"Training job name: {}\".format(model_job_name))\n",
    "print(\n",
    "    \"\\nInput Data Location: {}\".format(\n",
    "        training_params[\"InputDataConfig\"][0][\"DataSource\"][\"S3DataSource\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kick off the training job. \n",
    "\n",
    "Model training will take approximately about 1 hour to complete. You may check the training job status in SageMaker console or using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client = boto3.client(service_name=\"sagemaker\")\n",
    "sagemaker_client.create_training_job(**training_params)\n",
    "\n",
    "# Confirm that the training job has started\n",
    "status = sagemaker_client.describe_training_job(TrainingJobName=model_job_name)[\n",
    "    \"TrainingJobStatus\"\n",
    "]\n",
    "print(f\"Training job name: {model_job_name}\")\n",
    "print(\"Training job current status: {}\".format(status))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 200):\n",
    "    print(\n",
    "        \"Training job status: \",\n",
    "        sagemaker_client.describe_training_job(TrainingJobName=model_job_name)[\n",
    "            \"TrainingJobStatus\"\n",
    "        ],\n",
    "    )\n",
    "    print(\n",
    "        \"Secondary status: \",\n",
    "        sagemaker_client.describe_training_job(TrainingJobName=model_job_name)[\n",
    "            \"SecondaryStatus\"\n",
    "        ],\n",
    "    )\n",
    "    if (\n",
    "        sagemaker_client.describe_training_job(TrainingJobName=model_job_name)[\n",
    "            \"TrainingJobStatus\"\n",
    "        ]\n",
    "        == \"InProgress\"\n",
    "    ):\n",
    "        time.sleep(60)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a machine learning model\n",
    "\n",
    "On successful model training, SageMaker will create a model artifact in the S3 output path. We will now use the model artifact (from training) to create a deployable model in SageMaker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = sagemaker_client.describe_training_job(TrainingJobName=model_job_name)\n",
    "model_data = info[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "print(model_data)  # Model artifact\n",
    "\n",
    "primary_container = {\n",
    "    \"Image\": training_image,\n",
    "    \"ModelDataUrl\": model_data,\n",
    "}\n",
    "\n",
    "timestamp = time.strftime(\"-%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "model_name = \"sm-object-detection-demo\" + timestamp\n",
    "\n",
    "# Create a model from training artifact\n",
    "create_model_response = sagemaker_client.create_model(\n",
    "    ModelName=model_name, ExecutionRoleArn=role, PrimaryContainer=primary_container\n",
    ")\n",
    "\n",
    "print(create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model as endpoint for real time predictions\n",
    "\n",
    "Create endpoint configuration with name and instance type details. We will be using `ml.m4.xlarge` instance to host the model endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.strftime(\"-%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "endpoint_config_name = job_name_prefix + \"-epc\" + timestamp\n",
    "endpoint_config_response = sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.m4.xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint configuration name: {}\".format(endpoint_config_name))\n",
    "print(\n",
    "    \"Endpoint configuration arn:  {}\".format(\n",
    "        endpoint_config_response[\"EndpointConfigArn\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the endpoint and print the status. This step will take approximately 8mins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name_prefix = \"chest-xray-demo\"\n",
    "endpoint_name = job_name_prefix + \"-ep\" + timestamp\n",
    "print(\"Endpoint name: {}\".format(endpoint_name))\n",
    "\n",
    "endpoint_params = {\n",
    "    \"EndpointName\": endpoint_name,\n",
    "    \"EndpointConfigName\": endpoint_config_name,\n",
    "}\n",
    "endpoint_response = sagemaker_client.create_endpoint(**endpoint_params)\n",
    "print(\"EndpointArn = {}\".format(endpoint_response[\"EndpointArn\"]))\n",
    "\n",
    "# get the status of the endpoint\n",
    "response = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = response[\"EndpointStatus\"]\n",
    "print(\"EndpointStatus = {}\".format(status))\n",
    "\n",
    "# wait until the status has changed\n",
    "sagemaker_client.get_waiter(\"endpoint_in_service\").wait(EndpointName=endpoint_name)\n",
    "\n",
    "# print the status of the endpoint\n",
    "endpoint_response = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = endpoint_response[\"EndpointStatus\"]\n",
    "print(\"Endpoint creation ended with EndpointStatus = {}\".format(status))\n",
    "\n",
    "if status != \"InService\":\n",
    "    raise Exception(\"Endpoint creation failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Predictions\n",
    "\n",
    "In this section, we create 2 utility functions for visualizing the predictions. `make_predicted_image` is used to create a `BoxedImage` object from the ground truth utils class  and `perform_inference` is used to plot the image with actual predictions from SageMaker model. The `ground_truth_utils` library is from [here](https://github.com/aws/amazon-sagemaker-examples/blob/main/ground_truth_labeling_jobs/ground_truth_object_detection_tutorial/ground_truth_od.py).\n",
    "\n",
    "For making a realtime prediction, the image is first transformed and stored into an appropriate format. The image is then submitted to the model endpoint in the form of bytearray payload. Once the response (prediction) is received from model endpoint, the result is plotted for visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ground_truth_utils import BoundingBox, BoxedImage\n",
    "\n",
    "def make_predicted_image(predictions, img_id, uri):\n",
    "    ''' Creates a BoxedImage object with predicted bounding boxes. '''\n",
    "\n",
    "    img = BoxedImage(id=img_id, uri=uri)\n",
    "    img.download(f'./{local_dir}')\n",
    "    imread_img = img.imread()\n",
    "    imh, imw, *_ = imread_img.shape\n",
    "\n",
    "    # Create boxes.\n",
    "    for batch_data in predictions:\n",
    "        class_id, confidence, xmin, ymin, xmax, ymax = batch_data\n",
    "        boxdata = {'class_id': class_id,\n",
    "                   'height': (ymax-ymin)*imh,\n",
    "                   'width': (xmax-xmin)*imw,\n",
    "                   'left': xmin*imw,\n",
    "                   'top': ymin*imh}\n",
    "        box = BoundingBox(boxdata=boxdata, image_id=img.id)\n",
    "        img.consolidated_boxes.append(box)\n",
    "\n",
    "    return img\n",
    "\n",
    "def perform_inference(uri, local_dir):\n",
    "    '''Perform inference on an image'''\n",
    "\n",
    "    realtime_uri = uri\n",
    "    !aws s3 cp --quiet $realtime_uri data/the_image.png\n",
    "    test_image='data/the_image.png'\n",
    "    with open(test_image, 'rb') as f:\n",
    "        payload = f.read()\n",
    "        payload = bytearray(payload)\n",
    "\n",
    "    #manually set endpoint if job is interrupted\n",
    "    #endpoint_name='chest-xray-demo-ep-2020-07-24-01-48-42'\n",
    "    sm_runtime_client = boto3.client('sagemaker-runtime')\n",
    "    response = sm_runtime_client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                       ContentType='application/x-image', \n",
    "                                       Body=payload)\n",
    "\n",
    "    result = response['Body'].read()\n",
    "    result = json.loads(result)\n",
    "    predictions = [prediction for prediction in result['prediction'] if prediction[1] > .2]\n",
    "\n",
    "    realtime_img = make_predicted_image(predictions, 'RealtimeTest', realtime_uri)\n",
    "\n",
    "    # Plot the realtime prediction.\n",
    "    fig, ax = plt.subplots()\n",
    "    realtime_img.download(f'./{local_dir}')\n",
    "    realtime_img.plot_consolidated_bbs(ax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting images from validation and hold out datasets\n",
    "\n",
    "Pick 5 random images from the validation dataset and perform object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dir = \"validation\"\n",
    "f_in_validation = open(\"data/validation.manifest\", \"r\")\n",
    "with f_in_validation as f:\n",
    "    validation_uris = [json.loads(line.strip())[\"source-ref\"] for line in f.readlines()]\n",
    "validation_sample = np.random.choice(validation_uris, 3, replace=False)\n",
    "\n",
    "for i in range(0, len(validation_sample)):\n",
    "    perform_inference(validation_sample[i], local_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's pick 5 random images from the holdout set for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dir = \"holdout\"\n",
    "\n",
    "f_in_hold_out = open(\"data/hold_out.manifest\", \"r\")\n",
    "with f_in_hold_out as f:\n",
    "    hold_out_uris = [json.loads(line.strip())[\"source-ref\"] for line in f.readlines()]\n",
    "hold_out_uris_sample = np.random.choice(hold_out_uris, 3, replace=False)\n",
    "\n",
    "for i in range(0, len(hold_out_uris_sample)):\n",
    "    perform_inference(hold_out_uris_sample[i], local_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up \n",
    "\n",
    "Delete the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sagemaker_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
