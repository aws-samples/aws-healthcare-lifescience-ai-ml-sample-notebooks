{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HIV Inhibitor Prediction Using Graph Neural Networks (GNN) on Amazon SageMaker\n",
    "\n",
    "**Note:** This notebook was last tested with the `Python 3 (Pytorch 1.12 Python 3.8 CPU Optimized)` environment image in Amazon SageMaker Studio.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand the basics of graph neural networks and how they can be applied to molecular graphs\n",
    "- Install and use the Deep Graph Library (DGL)\n",
    "- Build, train, and deploy a DGL model on SageMaker\n",
    "- Perform hyperparameter tuning of deep learning models\n",
    "- Use your own scripts to train custom models in SageMaker \n",
    "- Track model training and other tasks using SageMaker Experiments\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Human immunodeficiency virus type 1 (HIV-1) is the most common cause of Acquired Immunodeficiency Syndrome (AIDS). One ongoing area of research is finding compounds that inhibit HIV-1 viral replication.  Schematically, this is shown below as:\n",
    "\n",
    "![Inhibitor](img/1.jpg)\n",
    "<div style=\"text-align: center\">\n",
    "<font size=1> \n",
    "Image Source : Biological evaluation of molecules of the azaBINOL class as antiviral agents (https://www.sciencedirect.com/science/article/abs/pii/S0968089619306704)\n",
    " </font>\n",
    "</div>\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "### Why is Deep Learning Useful for Analyzing Biological Networks?\n",
    "\n",
    "If you are familiar with classical network analysis, you may have encountered concepts such as the [betweenness centrality](https://en.wikipedia.org/wiki/Betweenness_centrality), [degree centrality](https://en.wikipedia.org/wiki/Centrality#Degree_centrality), or [random walk with restart](https://towardsdatascience.com/random-walks-with-restart-explained-77c3fe216bca). These methods are useful for calculating properties of nodes, analyzing networks, or grouping disease [genes](https://pubmed.ncbi.nlm.nih.gov/18371930/). However, these methods are **transductive,** which means that they can only generate features for a particular graph. They cannot predict edges, classify graphs, or perform other tasks where multiple graphs are needed. See [this](https://arxiv.org/abs/1706.02216) paper for further discussion of this issue.\n",
    "\n",
    "(A quick note on nomenclature: we use the term â€œgraphs\" to refer to biological networks; we reserve the term \"network\" for a neural network. Although it is common in the computational biology field to refer to biological graphs as networks, in the deep learning field, \"network\" refers almost exclusively to a neural network).\n",
    "\n",
    "[Convolution neural networks](https://www.d2l.ai/chapter_convolutional-neural-networks/index.html), commonly used in computer vision, are also useful for analyzing graphs. Convolutions allow for **inductive** learning, whereby features are learned for different graph topologies. These convolutions transform the underlying information in the graph nodes and edges. While a single convolutional layer is generally not sufficient for most tasks, deep graph convolutional neural networks can perform graph prediction (i.e., predict the class of a network), link prediction (predict missing edges in a network), and other tasks. \n",
    "\n",
    "Deep learning models can also incorporate different edge types as well as external information about edges and nodes. This makes deep learning an attractive approach for analyzing and making predictions about complex graphs. Biological networks are frequently very heterogeneous and include diverse data types such as metabolic, biophysical, proteomic and functional assays, and information about gene regulatory networks. For example, [this](https://www.amazon.science/blog/amazon-web-services-open-sources-biological-knowledge-graph-to-fight-covid-19) blog post shows how a knowledge graph with diverse node and edge types can predict drug repurposing.\n",
    "\n",
    "While scientists can create their own convolutional layers, deep learning researchers have already built many convolutions and architectures that have proven useful in many applications. For example, [GraphSage](https://arxiv.org/pdf/1706.02216.pdf) can predict protein-protein interactions. Another commonly used approach is [Graph Attention Networks](https://arxiv.org/pdf/1710.10903.pdf) (GAT).\n",
    "\n",
    "For a more details of deep graph learning and how it can help analyze biological data, see [this](https://academic.oup.com/bib/article/22/2/1515/5964185) review paper. You may also find [this](http://snap.stanford.edu/deepnetbio-ismb/) tutorial useful.\n",
    "\n",
    "### What is the Deep Graph Library (DGL) and When Should You Use It?\n",
    "\n",
    "Deep Graph Library (DGL) allows researches and developers to easily and quickly apply deep graph learning approaches to their data by abstracting away much of the difficult deep learning work and code. The DGL library comes with a number of prebuilt layers, including [GraphSage convolutions](https://docs.dgl.ai/generated/dgl.nn.pytorch.conv.SAGEConv.html#dgl.nn.pytorch.conv.SAGEConv), [GATs](https://docs.dgl.ai/generated/dgl.nn.pytorch.conv.GATConv.html#dgl.nn.pytorch.conv.GATConv), and [others](https://docs.dgl.ai/api/python/nn-pytorch.html). Users have have the flexibility to create their own layers and architectures as well.  \n",
    "\n",
    "The [DGL-LifeScience](https://lifesci.dgl.ai/index.html) Python package provides an even further abstraction of DGL, so that computational biologists, biochemists, and bioinformaticians who wish to leverage deep graph methods can easily do so for certain common use cases and performing common operations in the context of analyzing small and large molecules. If you want to learn more about how to use the DGL library, we recommend getting started with [this](https://docs.dgl.ai/en/0.6.x/guide/graph.html) tutorial.\n",
    "\n",
    "### Notebook Overview\n",
    "\n",
    "This example notebook trains multiple graph neural network models using Deep Graph Library and deploys them using Amazon SageMaker, a comprehensive and fully-managed machine learning service. With SageMaker, data scientists and developers can quickly and easily build and train machine learning models and then directly deploy them into a production-ready hosted environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by installing the latest version of `dgl` and other Python dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --disable-pip-version-check -U -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "import dgl\n",
    "\n",
    "from dgllife.model import load_pretrained\n",
    "from dgllife.utils import (\n",
    "    smiles_to_bigraph,\n",
    "    EarlyStopping,\n",
    "    Meter,\n",
    "    CanonicalAtomFeaturizer,\n",
    "    CanonicalBondFeaturizer,\n",
    ")\n",
    "from functools import partial\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dgllife.data import HIV\n",
    "\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "rdkit.__version__\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"DGLBACKEND\"] = \"pytorch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "node_featurizer = CanonicalAtomFeaturizer(atom_data_field=\"feat\")\n",
    "edge_featurizer = None\n",
    "num_workers = 1\n",
    "split_ratio = \"0.7:0.2:0.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Drug Therapeutics Program (DTP) AIDS Antiviral Screen](https://wiki.nci.nih.gov/display/NCIDTPdata/AIDS+Antiviral+Screen+Data) tested the ability of 43,850 compounds to inhibit viral replication. The DGL library has a pre-processed version of this dataset where each compound is classified as either Confirmed Inactive (CI; labeled as 0) or Confirmed Moderately Active/Confirmed Active (CM,CA; labeled as 1). You can download and inspect the raw dataset from [here](https://moleculenet.org/datasets-1). Alternatively, you can download a subset of the data focused on HIV [here](https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/HIV.csv). This data is in .csv format and provides the struture of the molecule (in [SMILES](https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system) format), the type of activity, and activity against HIV.\n",
    "\n",
    "\n",
    "|SMILES string |activity |HIV_active |\n",
    "|---  |---  |---  |\n",
    "|CC(C)(CCC(=O)O)CCC(=O)O  |CI |0  |\n",
    "|O=C(O)Cc1ccc(SSc2ccc(CC(=O)O)cc2)cc1 |CM |1  |\n",
    "|O=C(O)c1ccccc1SSc1ccccc1C(=O)O |CI |0  |\n",
    "|CCCCCCCCCCCC(=O)Nc1ccc(SSc2ccc(NC(=O)CCCCCCCCCCC)cc2)cc1 |CI |0  | \n",
    "\n",
    "Confirmed inactive (CI) compounds are labeled 0, while confirmed moderately active (CM)/confirmed active (CA) are labeled 1. We can use these labels to define our ML task as a **graph classification problem**. We will construct a graph to represent each molecule, considering the atoms as nodes and the chemical bonds between atoms as edges. Then, we will use GNN techniques to classify each molecule as either active or inactive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = HIV(\n",
    "    smiles_to_graph=partial(smiles_to_bigraph, add_self_loop=True),\n",
    "    node_featurizer=node_featurizer,\n",
    "    edge_featurizer=edge_featurizer,\n",
    "    n_jobs=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset contains around 41,000 molecules in `SMILES` format. The `HIV_active` column (label) indicates if the molecule inhibits HIV. Let's verify if there are any missing values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values in this dataset. Next, let's explore the class distribution of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.df[\"HIV_active\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.df[\"HIV_active\"].value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only ~3% compounds classified as HIV inhibtiors. This means that the dataset is heavily imbalaced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Molecular Properties With RDKit\n",
    "\n",
    "[RDKit](https://www.rdkit.org/docs/cppapi/index.html) is an open-source cheminformatics toolkit. It includes a collection of standard cheminformatics functions for molecule I/O, substructure searching, chemical reactions, 2D and 3D coordinate generation, fingerprinting, etc.\n",
    "\n",
    "We are going to use this library to explore the molecules in the dataset. First, let's randomly select and visualize several molecules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_molecule_start_index = 10\n",
    "\n",
    "sample_smiles = dataset.df[\"smiles\"][\n",
    "    random_molecule_start_index : random_molecule_start_index + 8\n",
    "].values\n",
    "sample_molecules = [Chem.MolFromSmiles(smile) for smile in sample_smiles]\n",
    "Draw.MolsToGridImage(sample_molecules, molsPerRow=4, subImgSize=(600, 600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's look at a single molecule and explore its properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_smiles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mol = sample_molecules[0]\n",
    "mol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the [RDKit::Atom](https://www.rdkit.org/docs/cppapi/classRDKit_1_1Atom.html) class to further explore the features of the molecules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atoms = mol.GetAtoms()\n",
    "print(\"Total number of atoms in the molecule : {}\".format(len(atoms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "molecule_features = []\n",
    "for atom in atoms:\n",
    "    atom_features = {}\n",
    "    atom_features[\"atomic_symbol\"] = atom.GetSymbol()\n",
    "    atom_features[\"atomic_numbers\"] = atom.GetAtomicNum()\n",
    "    atom_features[\"degree\"] = atom.GetDegree()\n",
    "    atom_features[\"formal_charge\"] = atom.GetFormalCharge()\n",
    "    atom_features[\"hybridization\"] = atom.GetHybridization()\n",
    "    atom_features[\"is_aromatic\"] = atom.GetIsAromatic()\n",
    "    molecule_features.append(atom_features)\n",
    "molecule_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we featurize the atoms of our molecule as nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atom_featurizer = CanonicalAtomFeaturizer(atom_data_field=\"feat\")\n",
    "atom_featurizer(mol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atom_featurizer.feat_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have these embedded in our graph since we used the [node featurizer](https://lifesci.dgl.ai/generated/dgllife.utils.CanonicalAtomFeaturizer.html) earlier in this notebook. \n",
    "\n",
    "Let's decode the graph associated to the above index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles, graphs, labels, masks = map(list, zip(*dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles[random_molecule_start_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_graph = graphs[random_molecule_start_index]\n",
    "\n",
    "random_graph.num_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_graph.num_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_graph.ndata[\"feat\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_graph.ndata[\"feat\"][0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_graph.ndata[\"feat\"][1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the order of the atoms represented in the graph is different. This will not be a problem for our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dgl-lifesci` package provides methods to split data into training, validation and test sets based on [several strategies](https://lifesci.dgl.ai/api/utils.splitters.html).\n",
    "\n",
    "We will use the [`ScaffoldSplitter`](https://lifesci.dgl.ai/api/utils.splitters.html#dgllife.utils.ScaffoldSplitter) for this project. This method groups molecules based on their scaffolds and sorts groups based on their sizes. The groups are then split for k-fold cross validation.\n",
    "\n",
    "As with other k-fold splitting methods, each molecule will appear only once in the validation set among all folds. In addition, this method ensures that molecules with the same scaffold will be collectively in either the training set or the validation set for each fold. Scaffold splitting, rather than random splitting, is commonly used in chemoinformatics to ensure the training and testing sets include similar molecules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgllife.utils import ScaffoldSplitter, RandomSplitter\n",
    "\n",
    "train_ratio, val_ratio, test_ratio = map(float, split_ratio.split(\":\"))\n",
    "\n",
    "train_set, val_set, test_set = ScaffoldSplitter.train_val_test_split(\n",
    "    dataset,\n",
    "    frac_train=train_ratio,\n",
    "    frac_val=val_ratio,\n",
    "    frac_test=test_ratio,\n",
    "    scaffold_func=\"smiles\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the distribution of classes of train, validation, test datasets after the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.dataset.df.loc[train_set.indices][\"HIV_active\"].value_counts().plot(\n",
    "    kind=\"bar\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set.dataset.df.loc[val_set.indices][\"HIV_active\"].value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.dataset.df.loc[test_set.indices][\"HIV_active\"].value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three datasets (train, test, validation) have a similar class distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Data to S3\n",
    "In order to accomodate model training on SageMaker we need to upload the data to s3 location. We are going to use the sagemaker.Session.upload_data function to upload our datasets to an S3 location. The return value inputs identifies the location -- we will use later when we start the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "s3_prefix = \"./hiv_inhibitor_prediction/sagemaker\"\n",
    "\n",
    "dataset.df.to_csv(\"full.csv\", index=False)\n",
    "pd.DataFrame(train_set.indices, columns=[\"indices\"]).to_csv(\"train.csv\", index=False)\n",
    "pd.DataFrame(val_set.indices, columns=[\"indices\"]).to_csv(\"validation.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_full = session.upload_data(path=\"full.csv\", bucket=bucket, key_prefix=s3_prefix)\n",
    "input_train = session.upload_data(path=\"train.csv\", bucket=bucket, key_prefix=s3_prefix)\n",
    "input_val = session.upload_data(\n",
    "    path=\"validation.csv\", bucket=bucket, key_prefix=s3_prefix\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see `GCNPredictor` architecture compised of multple layers of `gnn_layers` which itself comprised of DGL `GraphConv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on SageMaker\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Model Architecture\n",
    "\n",
    "We are going to represent each of the modelcule as a graph with each atom as a node. The atom properties will be the node features after doing the data transformations. We will then use these features to classify the whole graph/molecule as whether it inhibits HIV virus replication or not using graph neural networks (GNNs). In GNN terms this is considered as graph classification problem.\n",
    "\n",
    "We will use prebuilt GCN and GAT model architectures packaged with DGL-LifeSci to train the model. Please refer to the GCNPredictor [documentation](https://lifesci.dgl.ai/_modules/dgllife/model/model_zoo/gcn_predictor.html) and [code](https://github.com/awslabs/dgl-lifesci/blob/master/python/dgllife/model/model_zoo/gcn_predictor.py) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgllife.model import GCNPredictor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model = GCNPredictor(\n",
    "    in_feats=10, hidden_feats=[10, 4], activation=[F.relu, F.relu], residual=[False] * 2\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Training Script\n",
    "\n",
    "We are going to use Pytorch as the DGL backend. Our training script will save model training artifacts to a file path called `model_dir`, as defined by the SageMaker PyTorch image. When training is finished, SageMaker will upload the model artifacts saved in `model_dir` to S3 for later deployment.\n",
    "\n",
    "We save this script in a file named `code/train.py`. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycat code/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Experiment\n",
    "\n",
    "SageMaker Experiments enable us to organize related trials for later comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.experiments import Experiment\n",
    "from time import strftime\n",
    "\n",
    "\n",
    "experiment_name = f\"HIV-Inhibitor-Prediction-Experiment-{strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "try:\n",
    "    experiment = Experiment.load(experiment_name)\n",
    "except Exception as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == \"ResourceNotFound\":\n",
    "        print(\n",
    "            \"Experiment name [{}], does not exists. Hence creating.\".format(\n",
    "                experiment_name\n",
    "            )\n",
    "        )\n",
    "        experiment = Experiment.create(\n",
    "            experiment_name=experiment_name,\n",
    "            description=\"Experiment to track the HIV inhibitor prediction trials.\",\n",
    "        )\n",
    "    else:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define hyperparameters assocated with the model. As per the training script we created above here are some of the hyperparameters that we can use to tune our model(s). One highlight here is that the model architecture is also given here as the hyperparameter which allows other model architecutures like GAT or MPNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "import time\n",
    "\n",
    "hyperparameters = {\n",
    "    # Feature Engineering\n",
    "    \"gnn-featurizer-type\": \"canonical\",\n",
    "    # Model Architecture\n",
    "    \"gnn-model-name\": \"GCN-p\",\n",
    "    \"gnn-residuals\": False,\n",
    "    \"gnn-batchnorm\": True,\n",
    "    \"gnn-dropout\": 0.0013086019242321,\n",
    "    \"gnn-predictor-hidden-feats\": 512,\n",
    "    # Training\n",
    "    \"batch-size\": 512,\n",
    "    \"epochs\": 20,\n",
    "    \"learning-rate\": 0.000508635928951698,\n",
    "    \"weight-decay\": 0.0013253058161908312,\n",
    "    \"patience\": 30,\n",
    "}\n",
    "\n",
    "metric_definitions = [\n",
    "    {\n",
    "        \"Name\": \"train:roc_auc_score\",\n",
    "        \"Regex\": \"training:roc_auc_score\\s\\[([0-9\\\\,.]+)\\]\",\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"validation:roc_auc_score\",\n",
    "        \"Regex\": \",\\svalidation:roc_auc_score\\s\\[([0-9\\\\,.]+)\\]\",\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"best validation:roc_auc_score\",\n",
    "        \"Regex\": \"best\\svalidation:roc_auc_score\\s\\[([0-9\\\\,.]+)\\]\",\n",
    "    },\n",
    "    {\"Name\": \"epoch\", \"Regex\": \"epoch\\s\\[([0-9]+)\\]\"},\n",
    "    {\"Name\": \"train:loss\", \"Regex\": \"loss\\s\\[([0-9\\\\,.]+)\\]\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit Training Job\n",
    "\n",
    "DGL supports Tensorflow, PyTorch and MXNet runtimes. For for this project we are going to use `PyTorch` as the backend.\n",
    "\n",
    "The Amazon SageMaker Python SDK makes it easier to run a PyTorch script in Amazon SageMaker using its PyTorch estimator. We can also use the SageMaker Python SDK to deploy the trained model and run predictions. For more information on how to use this SDK with PyTorch, see the [SageMaker Python SDK documentation](https://sagemaker.readthedocs.io/en/stable/).\n",
    "\n",
    "To start, we use the PyTorch estimator class to train our model. When creating our estimator, we make sure to specify a few things:\n",
    "\n",
    "* `entry_point`: The name of our PyTorch script. It contains our training code which loads data from the input channels, configures training with hyperparameters, runs the training loop, and saves the model artifacts. It also contains code to load and run the model during inference.\n",
    "\n",
    "* `source_dir`: The location of our training scripts and `requirements.txt` file. The `requirements.txt` file lists packages you want to use with your script.\n",
    "\n",
    "* `framework_version`: The PyTorch version we want to use. The PyTorch estimator supports both single-machine & multi-machine, distributed PyTorch training using `SMDataParallel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.experiments.run import Run\n",
    "from time import strftime\n",
    "\n",
    "training_job_name = f\"Training-Job-{strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "print(\"Training job name: \", training_job_name)\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"code\",\n",
    "    role=role,\n",
    "    framework_version=\"1.9.0\",\n",
    "    py_version=\"py38\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.p3.2xlarge\",\n",
    "    debugger_hook_config=False,\n",
    "    disable_profiler=True,\n",
    "    hyperparameters=hyperparameters,\n",
    "    metric_definitions=metric_definitions,\n",
    ")\n",
    "\n",
    "with Run(\n",
    "    experiment_name=experiment_name,\n",
    "    sagemaker_session=session,\n",
    ") as run:\n",
    "    estimator.fit(\n",
    "        {\"data_full\": input_full, \"data_train\": input_train, \"data_val\": input_val},\n",
    "        job_name=training_job_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Training Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial training should produce a model with a validation AUC value between 75% and 80%. You can see more metrics like the training loss, validation loss, and ROC score over time in the Experiements view in SageMaker Studio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we deploy the model to an endpoint, let's see the where the trained model artifacts are stored in S3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = estimator.model_data\n",
    "print(\"Stored {} as model_data\".format(model_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the Model on Amazon SageMaker\n",
    "\n",
    "After training our model, we host it as an Amazon SageMaker Endpoint. We need to implement a few methods in `inference.py` for the endpoint to load the model and serve predictions correctly.\n",
    "\n",
    "* `model_fn()`: Loads the saved model and return a model object that can be used for model serving. The SageMaker PyTorch model server loads our model by invoking `model_fn`.\n",
    "* `input_fn()`: Deserializes and prepares the prediction input. In this example, our request body is first serialized to a JSON object containing SMILES strings for the target molecules. The `input_fn()` function first processes the graph using DGL. It then adds the features to each node using the same featurizer used during training. Finally, the function returns the graph with features in the format required by the model.\n",
    "* `predict_fn()`: Performs the prediction and returns the result. To deploy our endpoint, we call `deploy()` on our PyTorch estimator object, passing in our desired number of instances and instance type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Real-Time Inference Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "endpoint_name = \"HIV-Inhibitor-Prediction-EP-{}\".format(\n",
    "    time.strftime(\"%m-%d-%Y-%H-%M-%S\")\n",
    ")\n",
    "print(\"Endpoint name: \", endpoint_name)\n",
    "\n",
    "model = PyTorchModel(\n",
    "    model_data=model_data,\n",
    "    source_dir=\"code\",\n",
    "    entry_point=\"inference.py\",\n",
    "    role=role,\n",
    "    framework_version=\"1.9.0\",\n",
    "    py_version=\"py38\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = model.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.c5.xlarge\", endpoint_name=endpoint_name\n",
    ")\n",
    "\n",
    "predictor.serializer = sagemaker.serializers.JSONSerializer()\n",
    "predictor.deserializer = sagemaker.deserializers.JSONDeserializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Endpoint With a Single Target Molecule\n",
    "\n",
    "First, let's define a function to convert molecular data into SMILES format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_molgraphs(data):\n",
    "    \"\"\"Batching a list of datapoints for dataloader.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : list of 4-tuples.\n",
    "        Each tuple is for a single datapoint, consisting of\n",
    "        a SMILES, a DGLGraph, all-task labels and optionally a binary\n",
    "        mask indicating the existence of labels.\n",
    "    Returns\n",
    "    -------\n",
    "    smiles : list\n",
    "        List of SMILES strings\n",
    "    bg : DGLGraph\n",
    "        The batched DGLGraph.\n",
    "    labels : Tensor of dtype float32 and shape (B, T)\n",
    "        Batched datapoint labels. B is len(data) and\n",
    "        T is the number of total tasks.\n",
    "    masks : Tensor of dtype float32 and shape (B, T)\n",
    "        Batched datapoint binary mask, indicating the\n",
    "        existence of labels.\n",
    "    \"\"\"\n",
    "\n",
    "    smiles, graphs, labels, masks = map(list, zip(*data))\n",
    "\n",
    "    bg = dgl.batch(graphs)\n",
    "    bg.set_n_initializer(dgl.init.zero_initializer)\n",
    "    bg.set_e_initializer(dgl.init.zero_initializer)\n",
    "    labels = torch.stack(labels, dim=0)\n",
    "    masks = torch.stack(masks, dim=0)\n",
    "\n",
    "    return smiles, bg, labels, masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the `collate_molgraphs()` function to process the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_smiles, bg, test_labels, masks = collate_molgraphs(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our deployed endpoint to classify a single molecule from the test set. First, we select a random target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mol = Chem.MolFromSmiles(test_smiles[110])\n",
    "mol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's serialize the molecule data into JSON format and submit it to the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json = {\"smiles\": [test_smiles[110]]}\n",
    "\n",
    "prediction_logits = predictor.predict(json)\n",
    "prediction_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The endpoint response returns the logit value that our target molecule is an inhibitor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Endpoint With Multiple Targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json = {\"smiles\": test_smiles}\n",
    "\n",
    "prediction_logits = predictor.predict(json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the Test Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "roc_auc_score(test_labels[:, 0].numpy(), np.asarray(prediction_logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC AUC score for our initial testing should be around 0.75. Let's plot the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(test_labels[:, 0].numpy(), np.asarray(prediction_logits))\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.title(\"ROC Plot\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning \n",
    "\n",
    "So far we have trained a single model with fixed hyperparameters. Next let's try to further optimize the model using [Amazon SageMaker Hyperparameter Tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit Hyperparameter Tuning Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")\n",
    "\n",
    "gcn_hyperparameter_ranges = {\n",
    "    \"gnn-dropout\": ContinuousParameter(0.001, 0.003),\n",
    "    \"gnn-predictor-hidden-feats\": CategoricalParameter([128, 256, 512]),\n",
    "    \"batch-size\": CategoricalParameter([256, 512]),\n",
    "    \"learning-rate\": ContinuousParameter(0.0001, 0.001),\n",
    "    \"weight-decay\": ContinuousParameter(0.001, 0.01),\n",
    "}\n",
    "\n",
    "objective_metric_name = \"best validation:roc_auc_score\"\n",
    "\n",
    "gcn_estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"code\",\n",
    "    role=role,\n",
    "    framework_version=\"1.9.0\",\n",
    "    py_version=\"py38\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.p3.2xlarge\",\n",
    "    debugger_hook_config=False,\n",
    "    disable_profiler=True,\n",
    ")\n",
    "\n",
    "gcn_tuner = HyperparameterTuner(\n",
    "    gcn_estimator,\n",
    "    objective_metric_name,\n",
    "    gcn_hyperparameter_ranges,\n",
    "    metric_definitions,\n",
    "    max_jobs=1,\n",
    "    max_parallel_jobs=1,\n",
    ")\n",
    "\n",
    "hyper_parameter_job_name = \"hpo-hiv-gcn-p-{}\".format(time.strftime(\"%m-%d-%H-%M-%S\"))\n",
    "print(\"Training job name: \", hyper_parameter_job_name)\n",
    "\n",
    "gcn_tuner.fit(\n",
    "    {\"data_full\": input_full, \"data_train\": input_train, \"data_val\": input_val},\n",
    "    job_name=hyper_parameter_job_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your notebook loses its connection, you can reattach it by specifying the job name, like `gcn_tuner = HyperparameterTuner.attach(\"hpo-hiv-gcn-p-03-16-02-38-07\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Tuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the best model and its hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "smclient = boto3.client(\"sagemaker\")\n",
    "\n",
    "best_overall_training_job = smclient.describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=hyper_parameter_job_name\n",
    ")\n",
    "\n",
    "best_overall_training_job[\"BestTrainingJob\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gcn_training_job = sagemaker.estimator.Estimator.attach(\n",
    "    best_overall_training_job[\"BestTrainingJob\"][\"TrainingJobName\"]\n",
    ")\n",
    "best_gcn_model = PyTorchModel(\n",
    "    model_data=best_gcn_training_job.model_data,\n",
    "    source_dir=\"code\",\n",
    "    entry_point=\"inference.py\",\n",
    "    role=role,\n",
    "    framework_version=\"1.9.0\",\n",
    "    py_version=\"py38\",\n",
    ")\n",
    "\n",
    "best_gcn_predictor = best_gcn_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.c5.xlarge\",\n",
    "    endpoint_name=\"best-gcn-\" + endpoint_name,\n",
    ")\n",
    "best_gcn_predictor.serializer = sagemaker.serializers.JSONSerializer()\n",
    "best_gcn_predictor.deserializer = sagemaker.deserializers.JSONDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json = {\"smiles\": test_smiles}\n",
    "\n",
    "prediction_logits = best_gcn_predictor.predict(json)\n",
    "roc_auc_score(test_labels[:, 0].numpy(), np.asarray(prediction_logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up \n",
    "\n",
    "Lastly, please remember to delete the Amazon SageMaker endpoint to avoid charges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gcn_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for x in [\"full.csv\", \"train.csv\", \"validation.csv\", \"hiv_dglgraph.bin\"]:\n",
    "    os.remove(x)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.12 Python 3.8 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.12-cpu-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a8534c14445fc6cdc3039d8140510d6736e5b4960d89f445a45d8db6afd8452b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
