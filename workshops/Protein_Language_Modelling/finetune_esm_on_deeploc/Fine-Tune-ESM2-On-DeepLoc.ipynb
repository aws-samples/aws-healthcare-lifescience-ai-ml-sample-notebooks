{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee72f818-0ded-401d-b350-3dcb00346260",
   "metadata": {},
   "source": [
    "# Use Amazon SageMaker for Parameter-Efficient Fine Tuning of the ESM-2 Protein Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133f779d-16c6-48ae-bc6f-290855d42346",
   "metadata": {},
   "source": [
    "Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "SPDX-License-Identifier: MIT-0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea37b37-5093-44ec-9235-d8ba3186bb90",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Note: We recommend running this notebook on a **ml.t3.medium** instance with the **Data Science 3.0** image.\n",
    "\n",
    "### What is a Protein?\n",
    "\n",
    "Proteins are complex molecules that are essential for life. The shape and structure of a protein determines what it can do in the body. Knowing how a protein is folded and how it works helps scientists design drugs that target it. For example, if a protein causes disease, a drug might be made to block its function. The drug needs to fit into the protein like a key in a lock. Understanding the protein's molecular structure reveals where drugs can attach. This knowledge helps drive the discovery of innovative new drugs.\n",
    "\n",
    "![Proteins are made up of long chains of amino acids](../img/protein.png)\n",
    "\n",
    "### What is a Protein Language Model?\n",
    "\n",
    "Proteins are made up of linear chains of molecules called amino acids, each with its own chemical structure and properties. If we think of each amino acid in a protein like a word in a sentence, it becomes possible to analyze them using methods originally developed for analyzing human language. Scientists have trained these so-called, \"Protein Language Models\", or pLMs, on millions of protein sequences from thousands of organisms. With enough data, these models can begin to capture the underlying evolutionary relationships between different amino acid sequences.\n",
    "\n",
    "It can take a lot of time and compute to train a pLM from scratch for a certain task. For example, a team at Tsinghua University [recently described](https://www.biorxiv.org/content/10.1101/2023.07.05.547496v3) training a 100 Billion-parameter pLM on 768 A100 GPUs for 164 days! Fortunately, in many cases we can save time and resources by adapting an existing pLM to our needs. This technique is called \"fine-tuning\" and also allows us to borrow advanced tools from other types of language modeling\n",
    "\n",
    "### What is ESM-2?\n",
    "\n",
    "[ESM-2](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1) is a pLM trained using unsupervied masked language modelling on 250 Million protein sequences by researchers at [Facebook AI Research (FAIR)](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1). It is available in several sizes, ranging from 8 Million to 15 Billion parameters. The smaller models are suitable for various sequence and token classification tasks. The FAIR team also adapted the 3 Billion parameter version into the ESMFold protein structure prediction algorithm. They have since used ESMFold to predict the struture of [more than 700 Million metagenomic proteins](https://esmatlas.com/about). \n",
    "\n",
    "ESM-2 is a powerful pLM. However, it has traditionally required multiple A100 GPU chips to fine-tune. In this notebook, we demonstrate how to use QLoRA to fine-tune ESM-2 in on an inexpensive Amazon SageMaker training instance. We will use ESM-2 to predict [subcellular localization](https://academic.oup.com/nar/article/50/W1/W228/6576357). Understanding where proteins appear in cells can help us understand their role in disease and find new drug targets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d75cb75-18ed-4211-aa13-3edfccd59e4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## 1. Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f744fde-5624-46e2-b5a5-6d6dc1c58b4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:32:45.068665Z",
     "iopub.status.busy": "2025-04-29T16:32:45.068167Z",
     "iopub.status.idle": "2025-04-29T16:32:50.798025Z",
     "shell.execute_reply": "2025-04-29T16:32:50.796925Z",
     "shell.execute_reply.started": "2025-04-29T16:32:45.068635Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch==2.1.1+cpu in /opt/conda/lib/python3.11/site-packages (2.1.1+cpu)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch==2.1.1+cpu) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.11/site-packages (from torch==2.1.1+cpu) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch==2.1.1+cpu) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch==2.1.1+cpu) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch==2.1.1+cpu) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch==2.1.1+cpu) (2023.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch==2.1.1+cpu) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch==2.1.1+cpu) (1.3.0)\n",
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.11/site-packages (2.243.3)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (23.2.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.35.75 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (1.38.4)\n",
      "Requirement already satisfied: cloudpickle>=2.2.1 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: docker in /opt/conda/lib/python3.11/site-packages (from sagemaker) (7.1.0)\n",
      "Requirement already satisfied: fastapi in /opt/conda/lib/python3.11/site-packages (from sagemaker) (0.115.8)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.11/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (6.10.0)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.11/site-packages (from sagemaker) (4.23.0)\n",
      "Requirement already satisfied: numpy==1.26.4 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (1.26.4)\n",
      "Requirement already satisfied: omegaconf<=2.3,>=2.2 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (2.3.0)\n",
      "Requirement already satisfied: packaging<25,>=23.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (24.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from sagemaker) (2.2.3)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.11/site-packages (from sagemaker) (0.3.3)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.11/site-packages (from sagemaker) (4.3.6)\n",
      "Requirement already satisfied: protobuf<6.0,>=3.12 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (4.25.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from sagemaker) (5.9.8)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from sagemaker) (2.32.3)\n",
      "Requirement already satisfied: sagemaker-core<2.0.0,>=1.0.17 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (1.0.29)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.11/site-packages (from sagemaker) (0.7.7)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (2.0.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from sagemaker) (4.67.1)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (1.26.19)\n",
      "Requirement already satisfied: uvicorn in /opt/conda/lib/python3.11/site-packages (from sagemaker) (0.32.1)\n",
      "Requirement already satisfied: botocore<1.39.0,>=1.38.4 in /opt/conda/lib/python3.11/site-packages (from boto3<2.0,>=1.35.75->sagemaker) (1.38.4)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from boto3<2.0,>=1.35.75->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.13.0,>=0.12.0 in /opt/conda/lib/python3.11/site-packages (from boto3<2.0,>=1.35.75->sagemaker) (0.12.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.21.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /opt/conda/lib/python3.11/site-packages (from omegaconf<=2.3,>=2.2->sagemaker) (4.9.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker) (2.10.6)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker) (13.9.4)\n",
      "Requirement already satisfied: mock<5.0,>4.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker) (4.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.11/site-packages (from jsonschema->sagemaker) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.11/site-packages (from jsonschema->sagemaker) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from jsonschema->sagemaker) (0.23.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->sagemaker) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->sagemaker) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->sagemaker) (2025.1.31)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /opt/conda/lib/python3.11/site-packages (from fastapi->sagemaker) (0.45.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from fastapi->sagemaker) (4.12.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from google-pasta->sagemaker) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->sagemaker) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->sagemaker) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->sagemaker) (2025.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.9 in /opt/conda/lib/python3.11/site-packages (from pathos->sagemaker) (1.7.6.9)\n",
      "Requirement already satisfied: dill>=0.3.9 in /opt/conda/lib/python3.11/site-packages (from pathos->sagemaker) (0.4.0)\n",
      "Requirement already satisfied: pox>=0.3.5 in /opt/conda/lib/python3.11/site-packages (from pathos->sagemaker) (0.3.5)\n",
      "Requirement already satisfied: multiprocess>=0.70.17 in /opt/conda/lib/python3.11/site-packages (from pathos->sagemaker) (0.70.18)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.11/site-packages (from uvicorn->sagemaker) (8.1.8)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.11/site-packages (from uvicorn->sagemaker) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/conda/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (2.27.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (2.19.1)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /opt/conda/lib/python3.11/site-packages (from starlette<0.46.0,>=0.40.0->fastapi->sagemaker) (4.8.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.11/site-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi->sagemaker) (1.3.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers==4.35.2 in /opt/conda/lib/python3.11/site-packages (4.35.2)\n",
      "Requirement already satisfied: datasets==2.15.0 in /opt/conda/lib/python3.11/site-packages (2.15.0)\n",
      "Requirement already satisfied: s3fs==0.4.2 in /opt/conda/lib/python3.11/site-packages (0.4.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers==4.35.2) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.11/site-packages (from transformers==4.35.2) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers==4.35.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers==4.35.2) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers==4.35.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers==4.35.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers==4.35.2) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.11/site-packages (from transformers==4.35.2) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.11/site-packages (from transformers==4.35.2) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers==4.35.2) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets==2.15.0) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.11/site-packages (from datasets==2.15.0) (0.7)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets==2.15.0)\n",
      "  Using cached dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets==2.15.0) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets==2.15.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.11/site-packages (from datasets==2.15.0) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.15.0) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets==2.15.0) (3.9.5)\n",
      "Requirement already satisfied: botocore>=1.12.91 in /opt/conda/lib/python3.11/site-packages (from s3fs==0.4.2) (1.38.4)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from botocore>=1.12.91->s3fs==0.4.2) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.11/site-packages (from botocore>=1.12.91->s3fs==0.4.2) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.11/site-packages (from botocore>=1.12.91->s3fs==0.4.2) (1.26.19)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets==2.15.0) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets==2.15.0) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets==2.15.0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets==2.15.0) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets==2.15.0) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (4.12.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers==4.35.2) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers==4.35.2) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers==4.35.2) (2025.1.31)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==2.15.0)\n",
      "  Using cached multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "  Using cached multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets==2.15.0) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets==2.15.0) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.12.91->s3fs==0.4.2) (1.17.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp->datasets==2.15.0) (0.2.1)\n",
      "Using cached dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "Using cached multiprocess-0.70.15-py311-none-any.whl (135 kB)\n",
      "Installing collected packages: dill, multiprocess\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.4.0\n",
      "    Uninstalling dill-0.4.0:\n",
      "      Successfully uninstalled dill-0.4.0\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.18\n",
      "    Uninstalling multiprocess-0.70.18:\n",
      "      Successfully uninstalled multiprocess-0.70.18\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pathos 0.3.3 requires dill>=0.3.9, but you have dill 0.3.7 which is incompatible.\n",
      "pathos 0.3.3 requires multiprocess>=0.70.17, but you have multiprocess 0.70.15 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed dill-0.3.7 multiprocess-0.70.15\n"
     ]
    }
   ],
   "source": [
    "!pip install --disable-pip-version-check torch=='2.1.1+cpu' --index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install --upgrade sagemaker\n",
    "!pip install transformers=='4.35.2' datasets=='2.15.0' s3fs=='0.4.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffc3e030-375b-4d9d-94af-784899a0d99e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:32:50.800632Z",
     "iopub.status.busy": "2025-04-29T16:32:50.799553Z",
     "iopub.status.idle": "2025-04-29T16:32:55.352461Z",
     "shell.execute_reply": "2025-04-29T16:32:55.351676Z",
     "shell.execute_reply.started": "2025-04-29T16:32:50.800583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Fetched defaults config from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n",
      "s3://amazon-sagemaker-191384580845-us-east-1-3e7e9e6558a6/dzd_4efg23esvz9tyf/aromn82irdy0br/dev/\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "pipeline_session = PipelineSession()\n",
    "\n",
    "S3_BUCKET = sagemaker_session.default_bucket()\n",
    "\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "default_prefix = sagemaker_session.default_bucket_prefix\n",
    "smu_base_path = f\"s3://{default_bucket}/{default_prefix}/\"\n",
    "print(smu_base_path)\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "REGION_NAME = sagemaker_session.boto_region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7165d560-0aff-4f80-8234-937de96e5053",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:32:55.354390Z",
     "iopub.status.busy": "2025-04-29T16:32:55.353539Z",
     "iopub.status.idle": "2025-04-29T16:32:58.893354Z",
     "shell.execute_reply": "2025-04-29T16:32:58.892561Z",
     "shell.execute_reply.started": "2025-04-29T16:32:55.354352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assumed SageMaker role is arn:aws:iam::191384580845:role/datazone_usr_role_aromn82irdy0br_czv2v6qkj9v3jb\n",
      "S3 path is s3://amazon-sagemaker-191384580845-us-east-1-3e7e9e6558a6/esm-loc-ft\n",
      "Experiment name is esm-loc-ft-2025-04-29-16-32-58\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import sagemaker\n",
    "from sagemaker.experiments.run import Run\n",
    "from sagemaker.huggingface import HuggingFace, HuggingFaceModel\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from time import strftime\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "try:\n",
    "    sagemaker_execution_role = sagemaker_session.get_execution_role()\n",
    "except AttributeError:\n",
    "    NOTEBOOK_METADATA_FILE = \"/opt/ml/metadata/resource-metadata.json\"\n",
    "    with open(NOTEBOOK_METADATA_FILE, \"rb\") as f:\n",
    "        metadata = json.loads(f.read())\n",
    "        instance_name = metadata[\"ResourceName\"]\n",
    "        domain_id = metadata.get(\"DomainId\")\n",
    "        user_profile_name = metadata.get(\"UserProfileName\")\n",
    "        space_name = metadata.get(\"SpaceName\")\n",
    "    domain_desc = sagemaker_session.sagemaker_client.describe_domain(DomainId=domain_id)\n",
    "    if \"DefaultSpaceSettings\" in domain_desc:\n",
    "        sagemaker_execution_role = domain_desc[\"DefaultSpaceSettings\"][\"ExecutionRole\"]\n",
    "    else:\n",
    "        sagemaker_execution_role = domain_desc[\"DefaultUserSettings\"][\"ExecutionRole\"]\n",
    "\n",
    "print(f\"Assumed SageMaker role is {sagemaker_execution_role}\")\n",
    "\n",
    "S3_PREFIX = \"esm-loc-ft\"\n",
    "S3_PATH = sagemaker.s3.s3_path_join(\"s3://\", S3_BUCKET, S3_PREFIX)\n",
    "print(f\"S3 path is {S3_PATH}\")\n",
    "\n",
    "EXPERIMENT_NAME = \"esm-loc-ft-\" + strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "print(f\"Experiment name is {EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2497ac7-2c19-4668-9bf5-40c1636ce44b",
   "metadata": {},
   "source": [
    "Load the sagemaker package and create some service clients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09081bc9-1dd0-490c-a0b0-07207cbbaebc",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## 2. Build Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b083b46-c53a-4e3b-bd3a-11cb149b5ae2",
   "metadata": {},
   "source": [
    "We'll use a version of the [DeepLoc-2 data set](https://services.healthtech.dtu.dk/services/DeepLoc-2.0/) to fine tune our localization model. It consists of several thousand protein sequences, each with one or more experimentally-observed location labels. This data was extracted by the DeepLoc team at Technical University of Denmark from the public [UniProt sequence database](https://www.uniprot.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc12bb73-8673-4bf2-8f21-082751971b72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:32:58.903153Z",
     "iopub.status.busy": "2025-04-29T16:32:58.902924Z",
     "iopub.status.idle": "2025-04-29T16:33:00.954954Z",
     "shell.execute_reply": "2025-04-29T16:33:00.954200Z",
     "shell.execute_reply.started": "2025-04-29T16:32:58.903130Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Kingdom</th>\n",
       "      <th>Membrane</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MAAAAAAAAAAGAAGGRGSGPGRRRHLVPGAGGEAGEGAPGGAGDY...</td>\n",
       "      <td>Metazoa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MAAAAAAAAAAGAAGGRGSGPGRRRHLVPGAGGEAGEGAPGGAGDY...</td>\n",
       "      <td>Metazoa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MAAAAAAAAATEQQGSNGPVKKSMREKAVERRNVNKEHNSNFKAGY...</td>\n",
       "      <td>Metazoa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MAAAAAAAAATNGTGGSSGMEVDAAVVPSVMACGVTGSVSVALHPL...</td>\n",
       "      <td>Metazoa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MAAAAAAAGGAALAVSTGLETATLQKLALRRKKVLGAEEMELYELA...</td>\n",
       "      <td>Metazoa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28295</th>\n",
       "      <td>MYYFSRVAARTFCCCIFFCLATAYSRPDRNPRKIEKKDKKFFGASK...</td>\n",
       "      <td>Fungi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28297</th>\n",
       "      <td>MYYHNQHQGKSILSSSRMPISSERHPFLRGNGTGDSGLILSTDAKP...</td>\n",
       "      <td>Viridiplantae</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28298</th>\n",
       "      <td>MYYIGHPSYYRKHIEHVCFQHSGILKKRNYQKNQKKYIMKLNESAM...</td>\n",
       "      <td>Fungi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28300</th>\n",
       "      <td>MYYQNQHQGKNILSSSRMHITSERHPFLRGNSPGDSGLILSTDAKP...</td>\n",
       "      <td>Viridiplantae</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28301</th>\n",
       "      <td>MYYQNQHQGKNILSSSRMHITSERHPFLRGNSPGDSGLILSTDAKP...</td>\n",
       "      <td>Viridiplantae</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15876 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Sequence        Kingdom  \\\n",
       "0      MAAAAAAAAAAGAAGGRGSGPGRRRHLVPGAGGEAGEGAPGGAGDY...        Metazoa   \n",
       "1      MAAAAAAAAAAGAAGGRGSGPGRRRHLVPGAGGEAGEGAPGGAGDY...        Metazoa   \n",
       "3      MAAAAAAAAATEQQGSNGPVKKSMREKAVERRNVNKEHNSNFKAGY...        Metazoa   \n",
       "4      MAAAAAAAAATNGTGGSSGMEVDAAVVPSVMACGVTGSVSVALHPL...        Metazoa   \n",
       "6      MAAAAAAAGGAALAVSTGLETATLQKLALRRKKVLGAEEMELYELA...        Metazoa   \n",
       "...                                                  ...            ...   \n",
       "28295  MYYFSRVAARTFCCCIFFCLATAYSRPDRNPRKIEKKDKKFFGASK...          Fungi   \n",
       "28297  MYYHNQHQGKSILSSSRMPISSERHPFLRGNGTGDSGLILSTDAKP...  Viridiplantae   \n",
       "28298  MYYIGHPSYYRKHIEHVCFQHSGILKKRNYQKNQKKYIMKLNESAM...          Fungi   \n",
       "28300  MYYQNQHQGKNILSSSRMHITSERHPFLRGNSPGDSGLILSTDAKP...  Viridiplantae   \n",
       "28301  MYYQNQHQGKNILSSSRMHITSERHPFLRGNSPGDSGLILSTDAKP...  Viridiplantae   \n",
       "\n",
       "       Membrane  \n",
       "0             0  \n",
       "1             0  \n",
       "3             1  \n",
       "4             0  \n",
       "6             0  \n",
       "...         ...  \n",
       "28295         0  \n",
       "28297         0  \n",
       "28298         1  \n",
       "28300         0  \n",
       "28301         0  \n",
       "\n",
       "[15876 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kingdom\n",
      "Fungi            3614\n",
      "Metazoa          8571\n",
      "Other             387\n",
      "Viridiplantae    3304\n",
      "dtype: int64\n",
      "\n",
      "Membrane\n",
      "0    11175\n",
      "1     4701\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    \"https://services.healthtech.dtu.dk/services/DeepLoc-2.0/data/Swissprot_Train_Validation_dataset.csv\"\n",
    ").drop([\"Unnamed: 0\", \"Partition\"], axis=1)\n",
    "df[\"Membrane\"] = df[\"Membrane\"].astype(\"int32\")\n",
    "\n",
    "# filter for sequences between 100 and 512 amino acides\n",
    "df = df[df[\"Sequence\"].apply(lambda x: len(x)).between(100, 512)]\n",
    "\n",
    "# Remove unnecessary features\n",
    "df = df[[\"Sequence\", \"Kingdom\", \"Membrane\"]]\n",
    "\n",
    "display(df)\n",
    "print(df.groupby(\"Kingdom\").size())\n",
    "print()\n",
    "print(df.groupby(\"Membrane\").size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0557df21-6c2e-4f5a-9b22-67f993d6ebcf",
   "metadata": {},
   "source": [
    "This looks good, but the two membrane classes are unbalanced. We could resample our data and discard records from the majority class. However, this would reduce the total training data. Instead, we'll calculate the class weights during the training job and use them to adjust the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447f4051-8f4d-45bb-939e-5f24cf252022",
   "metadata": {},
   "source": [
    "Next, we tokenize the sequences and trim them to a max length of 512 amino acids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b941e433-ef3e-4d3b-9b73-19e2545eff6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:33:00.960382Z",
     "iopub.status.busy": "2025-04-29T16:33:00.960085Z",
     "iopub.status.idle": "2025-04-29T16:33:10.835078Z",
     "shell.execute_reply": "2025-04-29T16:33:10.833989Z",
     "shell.execute_reply.started": "2025-04-29T16:33:00.960352Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c9fd1f5e6a4dfebb0826f7ff1e590c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/12700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a4d59417734275840e1bfdd0d40678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/3176 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 12700\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 3176\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(df).train_test_split(test_size=0.2, shuffle=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
    "\n",
    "\n",
    "def preprocess_data(examples, max_length=512):\n",
    "    text = examples[\"Sequence\"]\n",
    "    encoding = tokenizer(text, truncation=True, max_length=max_length)\n",
    "    encoding[\"labels\"] = examples[\"Membrane\"]\n",
    "    return encoding\n",
    "\n",
    "\n",
    "encoded_dataset = dataset.map(\n",
    "    preprocess_data,\n",
    "    batched=True,\n",
    "    num_proc=os.cpu_count(),\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "encoded_dataset.set_format(\"torch\")\n",
    "print(encoded_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33176a99-52de-45f8-ab38-c4a86812a7bc",
   "metadata": {},
   "source": [
    "Look at an example record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "472268de-85c1-4ea9-894b-8d0e6087257a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:33:10.841051Z",
     "iopub.status.busy": "2025-04-29T16:33:10.840760Z",
     "iopub.status.idle": "2025-04-29T16:33:10.850653Z",
     "shell.execute_reply": "2025-04-29T16:33:10.849904Z",
     "shell.execute_reply.started": "2025-04-29T16:33:10.841024Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viewing example record 2350\n",
      "Raw sequence:\n",
      "<cls> M S G G G K G K H V G G K G G S K I G E R G Q M S H S A R A G L Q F P V G R V R R F L K A K T Q N N M R V G A K S A V Y S A A V L E Y L T A E V L E L A G N A A K D L K V K R I T P R H L Q L A I R G D E E L D T L I R A T I A G G G V L P H I N K Q L L I R T K E K Y P E E E E I I <eos>\n",
      "\n",
      "Tokenized sequence:\n",
      "[0, 20, 8, 6, 6, 6, 15, 6, 15, 21, 7, 6, 6, 15, 6, 6, 8, 15, 12, 6, 9, 10, 6, 16, 20, 8, 21, 8, 5, 10, 5, 6, 4, 16, 18, 14, 7, 6, 10, 7, 10, 10, 18, 4, 15, 5, 15, 11, 16, 17, 17, 20, 10, 7, 6, 5, 15, 8, 5, 7, 19, 8, 5, 5, 7, 4, 9, 19, 4, 11, 5, 9, 7, 4, 9, 4, 5, 6, 17, 5, 5, 15, 13, 4, 15, 7, 15, 10, 12, 11, 14, 10, 21, 4, 16, 4, 5, 12, 10, 6, 13, 9, 9, 4, 13, 11, 4, 12, 10, 5, 11, 12, 5, 6, 6, 6, 7, 4, 14, 21, 12, 17, 15, 16, 4, 4, 12, 10, 11, 15, 9, 15, 19, 14, 9, 9, 9, 9, 12, 12, 2]\n",
      "\n",
      "Label:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "random_idx = random.randint(3, len(encoded_dataset[\"train\"]))\n",
    "example = encoded_dataset[\"train\"][random_idx]\n",
    "\n",
    "print(f\"Viewing example record {random_idx}\")\n",
    "print(f\"Raw sequence:\\n{tokenizer.decode(example['input_ids'])}\\n\")\n",
    "print(f\"Tokenized sequence:\\n{example['input_ids'].tolist()}\\n\")\n",
    "print(f\"Label:\\n{example['labels']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519980a9-32ef-4b44-a2d5-ca54bcf89790",
   "metadata": {},
   "source": [
    "Finally, we upload the processed training, test, and validation data to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd371e0-2481-47c9-9c3b-4755fd51e4cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:33:10.855041Z",
     "iopub.status.busy": "2025-04-29T16:33:10.854351Z",
     "iopub.status.idle": "2025-04-29T16:33:11.964667Z",
     "shell.execute_reply": "2025-04-29T16:33:11.963872Z",
     "shell.execute_reply.started": "2025-04-29T16:33:10.855009Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484e548b0a31463c98bf96824ddeebec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/12700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5dbd87b148d4b8cac97d3fbc27c15af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3176 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_s3_uri = smu_base_path + \"esm/data/train\"\n",
    "test_s3_uri = smu_base_path + \"esm/data/test\"\n",
    "\n",
    "encoded_dataset[\"train\"].save_to_disk(train_s3_uri)\n",
    "encoded_dataset[\"test\"].save_to_disk(test_s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbc9cf1-76d0-4c01-9dc3-b4a0f0802a03",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## 3. Train model in SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5c1f99-4f6b-492c-9be0-973f941df9f4",
   "metadata": {},
   "source": [
    "Let's try a few different approaches to improving the efficiency of our fine-tuning job.\n",
    "\n",
    "### Gradient Accumulation\n",
    "\n",
    "Gradient accumulation is a training technique that allows models to simulate training on larger batch sizes. Typically, the batch size - the number of samples used to calculate the gradient in one training step - is limited by the GPU memory capacity. With gradient accumulation, the model calculates gradients on smaller batches first. Then, instead of updating the model weights right away, the gradients get accumulated over multiple small batches. Once the accumulated gradients equal the target larger batch size, the optimization step is performed to update the model. This lets models train with effectively bigger batches without exceeding the GPU memory limit. However, extra computation is needed for the smaller batch forward and backward passes. So increased batch sizes via gradient accumulation can slow down training, especially if too many accumulation steps are used. The aim is to maximize GPU usage but avoid excessive slowdowns from too many extra gradient computation steps.\n",
    "\n",
    "### Gradient Checkpointing\n",
    "\n",
    "Gradient checkpointing is a technique that reduces the memory needed during training while keeping the computational time reasonable. Large neural networks take up a lot of memory because they have to store all the intermediate values from the forward pass in order to calculate the gradients during the backward pass. This can cause memory issues. One solution is to not store these intermediate values, but then they have to be recalculated during the backward pass, which takes a lot of time. \n",
    "\n",
    "Gradient checkpointing provides a balanced approach. It saves only some of the intermediate values, called \"checkpoints,\" and recalculates the others as needed. So it uses less memory than storing everything, but also less computation than recalculating everything. By strategically selecting which activations to checkpoint, gradient checkpointing enables large neural networks to be trained with manageable memory usage and computation time. This important technique makes it feasible to train very large models that would otherwise run into memory limitations.\n",
    "\n",
    "### Low-Rank Adaptation of Large Language Models (LoRA)\n",
    "\n",
    "Large language models like ESM-2 can contain billions of parameters that are expensive to train and run. [Researchers](https://www.biorxiv.org/content/10.1101/2023.07.05.547496v3) have developed a new training method called Low-Rank Adaptation (LoRA) to make fine-tuning these huge models more efficient. \n",
    "\n",
    "The key idea behind LoRA is that when fine-tuning a model for a specific task, you don't need to update all of the original parameters. Instead, LoRA adds new smaller matrices to the model that transform the inputs and outputs. Only these smaller matrices are updated during fine-tuning, which is much faster and uses less memory. The original model parameters stay frozen. \n",
    "\n",
    "After fine-tuning with LoRA, the small adapted matrices can be merged back into the original model. Or they can be kept separate if you want to quickly fine-tune the model for other tasks without forgetting previous ones. Overall, LoRA allows large language models to be efficiently adapted to new tasks at a fraction of the usual cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dd96073-8068-460a-9176-d87f232d094a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:33:11.969864Z",
     "iopub.status.busy": "2025-04-29T16:33:11.969493Z",
     "iopub.status.idle": "2025-04-29T16:33:13.041585Z",
     "shell.execute_reply": "2025-04-29T16:33:13.040800Z",
     "shell.execute_reply.started": "2025-04-29T16:33:11.969732Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Applied value from config key = SageMaker.TrainingJob.VpcConfig.Subnets\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.TrainingJob.VpcConfig.SecurityGroupIds\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = {\n",
    "    \"model_id\": \"facebook/esm2_t33_650M_UR50D\",\n",
    "    \"epochs\": 1,\n",
    "    \"per_device_train_batch_size\": 8,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"use_gradient_checkpointing\": True,\n",
    "    \"lora\": True,\n",
    "}\n",
    "\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"epoch\", \"Regex\": \"'epoch': ([0-9.]*)\"},\n",
    "    {\n",
    "        \"Name\": \"max_gpu_mem\",\n",
    "        \"Regex\": \"Max GPU memory use during training: ([0-9.e-]*) MB\",\n",
    "    },\n",
    "    {\"Name\": \"train_loss\", \"Regex\": \"'loss': ([0-9.e-]*)\"},\n",
    "    {\n",
    "        \"Name\": \"train_samples_per_second\",\n",
    "        \"Regex\": \"'train_samples_per_second': ([0-9.e-]*)\",\n",
    "    },\n",
    "    {\"Name\": \"eval_loss\", \"Regex\": \"'eval_loss': ([0-9.e-]*)\"},\n",
    "    {\"Name\": \"eval_accuracy\", \"Regex\": \"'eval_accuracy': ([0-9.e-]*)\"},\n",
    "]\n",
    "\n",
    "hf_estimator = HuggingFace(\n",
    "    base_job_name=\"esm-2-membrane-ft\",\n",
    "    entry_point=\"lora-train.py\",\n",
    "    source_dir=\"scripts\",\n",
    "    instance_type=\"ml.p3.2xlarge\",\n",
    "    instance_count=1,\n",
    "    transformers_version=\"4.28\",\n",
    "    pytorch_version=\"2.0\",\n",
    "    py_version=\"py310\",\n",
    "    output_path=f\"{S3_PATH}/output\",\n",
    "    role=sagemaker_execution_role,\n",
    "    hyperparameters=hyperparameters,\n",
    "    metric_definitions=metric_definitions,\n",
    "    checkpoint_local_path=\"/opt/ml/checkpoints\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    tags=[{\"Key\": \"project\", \"Value\": \"esm-fine-tuning\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90f89d80-3b2f-4360-8a47-ccf5f0120891",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:39:13.574407Z",
     "iopub.status.busy": "2025-04-29T16:39:13.574109Z",
     "iopub.status.idle": "2025-04-29T16:39:16.590284Z",
     "shell.execute_reply": "2025-04-29T16:39:16.589277Z",
     "shell.execute_reply.started": "2025-04-29T16:39:13.574387Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with Run(\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ") as run:\n",
    "    hf_estimator.fit(\n",
    "        {\n",
    "            \"train\": TrainingInput(s3_data=train_s3_uri, input_mode=\"File\"),\n",
    "            \"test\": TrainingInput(s3_data=test_s3_uri, input_mode=\"File\"),\n",
    "        },\n",
    "        wait=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba110cf-b9fe-4e70-8443-671a3a8a88fc",
   "metadata": {},
   "source": [
    "You can view metrics and debugging information for this run in SageMaker Experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec1f3fe2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:39:21.253012Z",
     "iopub.status.busy": "2025-04-29T16:39:21.252742Z",
     "iopub.status.idle": "2025-04-29T16:39:21.444946Z",
     "shell.execute_reply": "2025-04-29T16:39:21.443697Z",
     "shell.execute_reply.started": "2025-04-29T16:39:21.252991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job name: esm-2-membrane-ft-2025-04-29-16-39-14-502\n",
      "Training job status: InProgress\n",
      "Training job output: None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TrialComponentName</th>\n",
       "      <td>esm-2-membrane-ft-2025-04-29-16-39-14-502-aws-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DisplayName</th>\n",
       "      <td>esm-2-membrane-ft-2025-04-29-16-39-14-502-aws-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SourceArn</th>\n",
       "      <td>arn:aws:sagemaker:us-east-1:191384580845:train...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SageMaker.ImageUri</th>\n",
       "      <td>763104351884.dkr.ecr.us-east-1.amazonaws.com/h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SageMaker.InstanceCount</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SageMaker.InstanceType</th>\n",
       "      <td>ml.p3.2xlarge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SageMaker.VolumeSizeInGB</th>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epochs</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient_accumulation_steps</th>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lora</th>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_id</th>\n",
       "      <td>\"facebook/esm2_t33_650M_UR50D\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>per_device_train_batch_size</th>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sagemaker_container_log_level</th>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sagemaker_job_name</th>\n",
       "      <td>\"esm-2-membrane-ft-2025-04-29-16-39-14-502\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sagemaker_program</th>\n",
       "      <td>\"lora-train.py\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sagemaker_region</th>\n",
       "      <td>\"us-east-1\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sagemaker_submit_directory</th>\n",
       "      <td>\"s3://amazon-sagemaker-191384580845-us-east-1-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use_gradient_checkpointing</th>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test - MediaType</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test - Value</th>\n",
       "      <td>s3://amazon-sagemaker-191384580845-us-east-1-3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train - MediaType</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train - Value</th>\n",
       "      <td>s3://amazon-sagemaker-191384580845-us-east-1-3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SageMaker.DebugHookOutput - MediaType</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SageMaker.DebugHookOutput - Value</th>\n",
       "      <td>s3://amazon-sagemaker-191384580845-us-east-1-3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trials</th>\n",
       "      <td>[Default-Run-Group-esm-loc-ft-2025-04-29-16-32...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Experiments</th>\n",
       "      <td>[esm-loc-ft-2025-04-29-16-32-58]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                       0\n",
       "TrialComponentName                     esm-2-membrane-ft-2025-04-29-16-39-14-502-aws-...\n",
       "DisplayName                            esm-2-membrane-ft-2025-04-29-16-39-14-502-aws-...\n",
       "SourceArn                              arn:aws:sagemaker:us-east-1:191384580845:train...\n",
       "SageMaker.ImageUri                     763104351884.dkr.ecr.us-east-1.amazonaws.com/h...\n",
       "SageMaker.InstanceCount                                                              1.0\n",
       "SageMaker.InstanceType                                                     ml.p3.2xlarge\n",
       "SageMaker.VolumeSizeInGB                                                            30.0\n",
       "epochs                                                                               1.0\n",
       "gradient_accumulation_steps                                                          4.0\n",
       "lora                                                                                true\n",
       "model_id                                                  \"facebook/esm2_t33_650M_UR50D\"\n",
       "per_device_train_batch_size                                                          8.0\n",
       "sagemaker_container_log_level                                                       20.0\n",
       "sagemaker_job_name                           \"esm-2-membrane-ft-2025-04-29-16-39-14-502\"\n",
       "sagemaker_program                                                        \"lora-train.py\"\n",
       "sagemaker_region                                                             \"us-east-1\"\n",
       "sagemaker_submit_directory             \"s3://amazon-sagemaker-191384580845-us-east-1-...\n",
       "use_gradient_checkpointing                                                          true\n",
       "test - MediaType                                                                    None\n",
       "test - Value                           s3://amazon-sagemaker-191384580845-us-east-1-3...\n",
       "train - MediaType                                                                   None\n",
       "train - Value                          s3://amazon-sagemaker-191384580845-us-east-1-3...\n",
       "SageMaker.DebugHookOutput - MediaType                                               None\n",
       "SageMaker.DebugHookOutput - Value      s3://amazon-sagemaker-191384580845-us-east-1-3...\n",
       "Trials                                 [Default-Run-Group-esm-loc-ft-2025-04-29-16-32...\n",
       "Experiments                                             [esm-loc-ft-2025-04-29-16-32-58]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "\n",
    "training_job_details = hf_estimator.latest_training_job.describe()\n",
    "print(f\"Training job name: {training_job_details.get('TrainingJobName')}\")\n",
    "print(f\"Training job status: {training_job_details.get('TrainingJobStatus')}\")\n",
    "print(f\"Training job output: {training_job_details.get('ModelArtifacts')}\")\n",
    "\n",
    "search_expression = {\n",
    "    \"Filters\": [\n",
    "        {\n",
    "            \"Name\": \"DisplayName\",\n",
    "            \"Operator\": \"Contains\",\n",
    "            \"Value\": \"Training\",\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "trial_component_analytics = ExperimentAnalytics(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    search_expression=search_expression,\n",
    ")\n",
    "\n",
    "trial_component_analytics.dataframe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3260c9d8-bffb-465d-a60f-4886d0e7586f",
   "metadata": {},
   "source": [
    "The following table compares the different training methods we discussed above and their effect on the run time, accuracy, and GPU memory requirements of our job.\n",
    "\n",
    "| Configuration | Billable time (sec) | Evaluation Accuracy | Max GPU Memory Usage (GB)\n",
    "| ----------- | ----------- | ----------- | ----------- |\n",
    "| Base 650M model | 1698 | 0.91 | 22.6 |\n",
    "| Base + Gradient Accumulation | 1266 | 0.90 | 17.8 |\n",
    "| Base + Gradient Checkpointing | 1753 | 0.91 | 10.2|\n",
    "| Base + LoRA | 1362 | 0.90 | 18.6 |\n",
    "\n",
    "\n",
    "All of the methods produced models with high evaluation accuracy. Using LoRA decreased the run time (and cost) by 32% and using gradient checkpointing decreased the maximum GPU memory usage by 60%. Depending on our constraints (cost, time, hardware) one of these approaches may make more sense than another.\n",
    "\n",
    "Each of these methods perform well by themselves, but what happens when we use them in combination? Here are the results:\n",
    "\n",
    "| Configuration | Billable time (sec) | Evaluation Accuracy | Max GPU Memory Usage (GB)\n",
    "| ----------- | ----------- | ----------- | ----------- |\n",
    "| Base + LoRA + GA + GC | 689 | 0.80 | 3.3 |\n",
    "\n",
    "In this case, we see a 12% reduction in accuracy. However, the run time improvements are very close to what we saw with LoRA by itself. More importantly, we've reduced the GPU memory use by 85%! This is a massive decrease that allows us to train on a wide range of cost-effective instance types.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca12bb0-8bec-4e33-b2a6-0542ba13b50e",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Deploy Model as Real-Time Inference Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f188d1c-fad8-4760-a4d1-8720d3082bb0",
   "metadata": {},
   "source": [
    "Finally, let's deploy our trained model to an inference endpoint and test it against some protein sequences with known subcellular localization. In this case, we'll load a previously-trained version of the model saved on the public HuggingFace model hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e17487-bd97-4cc6-a116-06945d15402c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:39:25.572465Z",
     "iopub.status.busy": "2025-04-29T16:39:25.572103Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Applied value from config key = SageMaker.Model.VpcConfig\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n",
      "--"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "hub = {\"HF_MODEL_ID\": \"bloyal/esm2_650M_membrane_loc\", \"HF_TASK\": \"text-classification\"}\n",
    "\n",
    "hf_model = HuggingFaceModel(\n",
    "    env=hub,\n",
    "    role=sagemaker_execution_role,\n",
    "    transformers_version=\"4.28\",\n",
    "    pytorch_version=\"2.0\",\n",
    "    py_version=\"py310\",\n",
    ")\n",
    "\n",
    "predictor = hf_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.r5.2xlarge\",\n",
    "    role=sagemaker_execution_role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a9152d-7537-443f-8348-3e766f0e4d42",
   "metadata": {},
   "source": [
    "Try running some known proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75ec68e-b12f-4dc6-a58b-947cc494ad5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example cell membrane proteins\n",
    "glp_1_receptor = \"MAGAPGPLRLALLLLGMVGRAGPRPQGATVSLWETVQKWREYRRQCQRSLTEDPPPATDLFCNRTFDEYACWPDGEPGSFVNVSCPWYLPWASSVPQGHVYRFCTAEGLWLQKDNSSLPWRDLSECEESKRGERSSPEEQLLFLYIIYTVGYALSFSALVIASAILLGFRHLHCTRNYIHLNLFASFILRALSVFIKDAALKWMYSTAAQQHQWDGLLSYQDSLSCRLVFLLMQYCVAANYYWLLVEGVYLYTLLAFSVLSEQWIFRLYVSIGWGVPLLFVVPWGIVKYLYEDEGCWTRNSNMNYWLIIRLPILFAIGVNFLIFVRVICIVVSKLKANLMCKTDIKCRLAKSTLTLIPLLGTHEVIFAFVMDEHARGTLRFIKLFTELSFTSFQGLMVAILYCFVNNEVQLEFRKSWERWRLEHLHIQRDSSMKPLKCPTSSLSSGATAGSSMYTATCQASCS\"\n",
    "pd1 = \"MQIPQAPWPVVWAVLQLGWRPGWFLDSPDRPWNPPTFSPALLVVTEGDNATFTCSFSNTSESFVLNWYRMSPSNQTDKLAAFPEDRSQPGQDCRFRVTQLPNGRDFHMSVVRARRNDSGTYLCGAISLAPKAQIKESLRAELRVTERRAEVPTAHPSPSPRPAGQFQTLVVGVVGGLLGSLVLLVWVLAVICSRAARGTIGARRTGQPLKEDPSAVPVFSVDYGELDFQWREKTPEPPVPCVPEQTEYATIVFPSGMGTSSPARRGSADGPRSAQPLRPEDGHCSWPL\"\n",
    "rit1 = \"MDSGTRPVGSCCSSPAGLSREYKLVMLGAGGVGKSAMTMQFISHRFPEDHDPTIEDAYKIRIRIDDEPANLDILDTAGQAEFTAMRDQYMRAGEGFIICYSITDRRSFHEVREFKQLIYRVRRTDDTPVVLVGNKSDLKQLRQVTKEEGLALAREFSCPFFETSAAYRYYIDDVFHALVREIRRKEKEAVLAMEKKSKPKNSVWKRLKSPFRKKKDSVT\"\n",
    "\n",
    "# Example non-cell membrane proteins\n",
    "tubulin_beta_1 = \"MREIVHIQIGQCGNQIGAKFWEMIGEEHGIDLAGSDRGASALQLERISVYYNEAYGRKYVPRAVLVDLEPGTMDSIRSSKLGALFQPDSFVHGNSGAGNNWAKGHYTEGAELIENVLEVVRHESESCDCLQGFQIVHSLGGGTGSGMGTLLMNKIREEYPDRIMNSFSVMPSPKVSDTVVEPYNAVLSIHQLIENADACFCIDNEALYDICFRTLKLTTPTYGDLNHLVSLTMSGITTSLRFPGQLNADLRKLAVNMVPFPRLHFFMPGFAPLTAQGSQQYRALSVAELTQQMFDARNTMAACDLRRGRYLTVACIFRGKMSTKEVDQQLLSVQTRNSSCFVEWIPNNVKVAVCDIPPRGLSMAATFIGNNTAIQEIFNRVSEHFSAMFKRKAFVHWYTSEGMDINEFGEAENNIHDLVSEYQQFQDAKAVLEEDEEVTEEAEMEPEDKGH\"\n",
    "p53 = \"MEEPQSDPSVEPPLSQETFSDLWKLLPENNVLSPLPSQAMDDLMLSPDDIEQWFTEDPGPDEAPRMPEAAPPVAPAPAAPTPAAPAPAPSWPLSSSVPSQKTYQGSYGFRLGFLHSGTAKSVTCTYSPALNKMFCQLAKTCPVQLWVDSTPPPGTRVRAMAIYKQSQHMTEVVRRCPHHERCSDSDGLAPPQHLIRVEGNLRVEYLDDRNTFRHSVVVPYEPPEVGSDCTTIHYNYMCNSSCMGGMNRRPILTIITLEDSSGNLLGRNSFEVRVCACPGRDRRTEEENLRKKGEPHHELPPGSTKRALPNNTSSSPQPKKKPLDGEYFTLQIRGRERFEMFRELNEALELKDAQAGKEPGGSRAHSSHLKSKKGQSTSRHKKLMFKTEGPDSD\"\n",
    "adh5 = \"MANEVIKCKAAVAWEAGKPLSIEEIEVAPPKAHEVRIKIIATAVCHTDAYTLSGADPEGCFPVILGHEGAGIVESVGEGVTKLKAGDTVIPLYIPQCGECKFCLNPKTNLCQKIRVTQGKGLMPDGTSRFTCKGKTILHYMGTSTFSEYTVVADISVAKIDPLAPLDKVCLLGCGISTGYGAAVNTAKLEPGSVCAVFGLGGVGLAVIMGCKVAGASRIIGVDINKDKFARAKEFGATECINPQDFSKPIQEVLIEMTDGGVDYSFECIGNVKVMRAALEACHKGWGVSVVVGVAASGEEIATRPFQLVTGRTWKGTAFGGWKSVESVPKLVSEYMSKKIKVDEFVTHNLSFDEINKAFELMHSGKSIRTVVKI\"\n",
    "\n",
    "sample = {\"inputs\": glp_1_receptor}\n",
    "predictor.predict(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c03974a",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Clean  up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1473b8b6",
   "metadata": {},
   "source": [
    "Delete endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f4e10a-0152-4e03-931c-cb2820a8e337",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    predictor.delete_endpoint()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203627a2-1573-41b4-9aa7-9d8e945d1384",
   "metadata": {},
   "source": [
    "Delete S3 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910fdbcf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "boto_session = boto3.session.Session()\n",
    "bucket = boto_session.resource(\"s3\").Bucket(S3_BUCKET)\n",
    "bucket.objects.filter(Prefix=S3_PREFIX).delete()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
