{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10f42322-8e6b-4df5-9585-a969d364b12a",
   "metadata": {},
   "source": [
    "# Summarize Scientific Documents with Amazon Comprehend and HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e28edf-de16-4de0-96ff-c4b5678d4e17",
   "metadata": {},
   "source": [
    "Researchers must stay up-to-date on their fields of interest. However, it's difficult to keep track of the large number of journals, whitepapers, and research pre-prints generated in many areas. In response, many research groups have turned to AI/ML tools to summarize and classify new documents.\n",
    "\n",
    "In this workshop, we'll use several AWS AI/ML services to process scientific documents from the [NIH NCBI PMC Article Dataset](https://registry.opendata.aws/ncbi-pmc/) on the Registry of Open Data. This is a free full-text archive of biomedical and life sciences journal article at the U.S. National Institutes of Health's National Library of Medicine.\n",
    "\n",
    "NOTE: This notebook requires that the SageMaker Execution Role has additional permission to call the Amazon Comprehend services. Please reach out to your system administrator if you are running this outside of an AWS-hosted workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb6a24c-797e-4c3f-b109-1353407857a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --force-reinstall boto3\n",
    "!pip install --upgrade --force-reinstall sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da45f90-8e9c-4c0e-8fe6-da0f9bbe994b",
   "metadata": {},
   "source": [
    "# 1. Import Libraries and Create Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f953c1-9862-42b4-b195-6ad821562eb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import os\n",
    "import json\n",
    "import pprint\n",
    "import pandas as pd\n",
    "from random import sample\n",
    "import re\n",
    "\n",
    "boto_session = boto3.Session()\n",
    "s3 = boto_session.client(\"s3\")\n",
    "sm_session = sagemaker.Session(boto_session=boto_session)\n",
    "s3_bucket = sm_session.default_bucket()\n",
    "s3_prefix = \"sci-docs/data\"\n",
    "print(f\"S3 path is {s3_bucket}/{s3_prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de31b09-df82-49af-876f-f5c76b4dd13c",
   "metadata": {},
   "source": [
    "# 2. Download Documents from the NIH NCBI PMC Article Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7ac0c3-4b3d-48cb-84be-388f00f758ee",
   "metadata": {},
   "source": [
    "Copy 25 articles from the PubMed open data set (https://registry.opendata.aws/ncbi-pmc/) into the SageMaker default bucket for this account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ba4ce3-8f55-45b1-8acc-f0146e51a1db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pmc_bucket = \"pmc-oa-opendata\"\n",
    "pmc_prefix = \"oa_comm/txt/all/\"\n",
    "local_raw_data_dir = \"data/raw\"\n",
    "#let us specify 25 specific PMC articles to download\n",
    "article_names=[\n",
    "\"PMC1043862.txt\",\n",
    "\"PMC1054881.txt\",\n",
    "\"PMC1054888.txt\",\n",
    "\"PMC1064076.txt\",\n",
    "\"PMC1064081.txt\",\n",
    "\"PMC1064104.txt\",\n",
    "\"PMC1064852.txt\",\n",
    "\"PMC1064855.txt\",\n",
    "\"PMC1064860.txt\",\n",
    "\"PMC1064883.txt\",\n",
    "\"PMC1064892.txt\",\n",
    "\"PMC1064893.txt\",\n",
    "\"PMC1065049.txt\",\n",
    "\"PMC1065056.txt\",\n",
    "\"PMC1065057.txt\",\n",
    "\"PMC1065073.txt\",\n",
    "\"PMC1065100.txt\",\n",
    "\"PMC1065320.txt\",\n",
    "\"PMC1065326.txt\",\n",
    "\"PMC1069647.txt\",\n",
    "\"PMC1069665.txt\",\n",
    "\"PMC1073698.txt\",\n",
    "\"PMC1074343.txt\",\n",
    "\"PMC1074358.txt\",\n",
    "\"PMC1074751.txt\",\n",
    "]\n",
    "for article in article_names:\n",
    "    print(article)\n",
    "    sm_session.download_data(\n",
    "        local_raw_data_dir, bucket=pmc_bucket, key_prefix=pmc_prefix + article\n",
    "    )\n",
    "    \n",
    "# Once all files have been downloaded, upload them all to the S3 bucket for your project\n",
    "sm_session.upload_data(\n",
    "    local_raw_data_dir, bucket=s3_bucket, key_prefix=s3_prefix + \"/raw\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218c3891-39c2-4db0-ad41-897c802971f9",
   "metadata": {},
   "source": [
    "Look at a few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf08584a-adb6-4988-8186-9f42a2387527",
   "metadata": {},
   "outputs": [],
   "source": [
    "art = sample(article_names, 1)[0]\n",
    "print(art)\n",
    "!head data/raw/{art}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5548da-8719-43a7-9480-9efda9f5fecc",
   "metadata": {},
   "source": [
    "# 3. Summarize the Documents Using Amazon Comprehend Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bed8d3-6fe9-44cc-9e1a-f9b7951f0b74",
   "metadata": {},
   "source": [
    "Submit an Amazon Comprehend topic modelling job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2984d622-2a9f-42db-9d70-29c25c941d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehend = boto_session.client(service_name=\"comprehend\")\n",
    "\n",
    "sagemaker.s3.s3_path_join(s3_bucket, s3_prefix, \"raw\")\n",
    "\n",
    "input_s3_url = sagemaker.s3.s3_path_join(\"s3://\", s3_bucket, s3_prefix, \"raw\")\n",
    "input_doc_format = \"ONE_DOC_PER_FILE\"\n",
    "output_s3_url = sagemaker.s3.s3_path_join(\"s3://\", s3_bucket, s3_prefix, \"output\")\n",
    "data_access_role_arn = sagemaker.session.get_execution_role()\n",
    "number_of_topics = 25\n",
    "\n",
    "input_data_config = {\"S3Uri\": input_s3_url, \"InputFormat\": input_doc_format}\n",
    "output_data_config = {\"S3Uri\": output_s3_url}\n",
    "\n",
    "start_topics_detection_job_result = comprehend.start_topics_detection_job(\n",
    "    NumberOfTopics=number_of_topics,\n",
    "    InputDataConfig=input_data_config,\n",
    "    OutputDataConfig=output_data_config,\n",
    "    DataAccessRoleArn=data_access_role_arn,\n",
    ")\n",
    "\n",
    "job_id = start_topics_detection_job_result[\"JobId\"]\n",
    "print(f\"Job {job_id} submitted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea15645d-efa1-48e1-b666-0c819fdb865e",
   "metadata": {},
   "source": [
    "Once job is finished, download and unpack the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc63594d-d585-424c-8150-b017066b4e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_topics_detection_job_result = comprehend.describe_topics_detection_job(\n",
    "    JobId=job_id\n",
    ").get(\"TopicsDetectionJobProperties\", [])\n",
    "print(f\"Job {job_id} status is {describe_topics_detection_job_result['JobStatus']}\")\n",
    "\n",
    "if describe_topics_detection_job_result[\"JobStatus\"] == \"COMPLETED\":\n",
    "    output_url = sagemaker.s3.parse_s3_url(\n",
    "        describe_topics_detection_job_result[\"OutputDataConfig\"][\"S3Uri\"]\n",
    "    )\n",
    "    sm_session.download_data(\n",
    "        \"data\",\n",
    "        bucket=output_url[0],\n",
    "        key_prefix=output_url[1],\n",
    "    )\n",
    "    os.system(\"tar xvfz data/output.tar.gz -C data/\")\n",
    "\n",
    "    topics = (\n",
    "        pd.read_csv(\"data/topic-terms.csv\")\n",
    "        .sort_values([\"topic\", \"weight\"], ascending=[True, False])\n",
    "        .groupby([\"topic\"])[\"term\"]\n",
    "        .agg(lambda x: \", \".join(x))\n",
    "    )\n",
    "    docs = pd.read_csv(\"data/doc-topics.csv\").sort_values(\n",
    "        [\"docname\", \"proportion\"], ascending=[True, False]\n",
    "    )\n",
    "    results = pd.merge(docs, topics, how=\"left\", on=\"topic\")\n",
    "    display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcae0b6-c17a-4304-a448-10f94d6fc3cc",
   "metadata": {},
   "source": [
    "Let's look at some specific examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4f4ddb-1fc9-4fc7-9df0-6ab9e209288d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if describe_topics_detection_job_result[\"JobStatus\"] == \"COMPLETED\":\n",
    "\n",
    "    input_url = sagemaker.s3.parse_s3_url(\n",
    "        describe_topics_detection_job_result[\"InputDataConfig\"][\"S3Uri\"]\n",
    "    )\n",
    "    sample = results.sample()\n",
    "    docname, idx, score, terms = sample.iloc[0, :]\n",
    "\n",
    "    print(f\"Document name is {docname}\")\n",
    "    print(f\"Identified terms are {terms}\")\n",
    "\n",
    "    sm_session.download_data(\n",
    "        \"data\", bucket=input_url[0], key_prefix=os.path.join(input_url[1], docname)\n",
    "    )\n",
    "    os.system(f\"head -n 25 data/{docname}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80681160-180a-4fce-8ce1-cc79899aeae0",
   "metadata": {},
   "source": [
    "# 4. Generate TLDR Summaries Using a Pre-Trained NLP Model from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785636a6-6033-469c-b3fb-7522fc6248ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker.async_inference.async_inference_config import AsyncInferenceConfig\n",
    "from sagemaker.async_inference.waiter_config import WaiterConfig\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\"HF_MODEL_ID\": \"alk/pegasus-scitldr\", \"HF_TASK\": \"text2text-generation\"}\n",
    "\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    transformers_version=\"4.17.0\",\n",
    "    pytorch_version=\"1.10.2\",\n",
    "    py_version=\"py38\",\n",
    "    env=hub,\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d918044-8196-43cf-8f53-214985758539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy model to SageMaker Inference\n",
    "async_config = AsyncInferenceConfig(\n",
    "    output_path=f\"s3://{s3_bucket}/{s3_prefix}/tldr_output\",\n",
    "    max_concurrent_invocations_per_instance=4,\n",
    ")\n",
    "\n",
    "predictor = huggingface_model.deploy(\n",
    "    async_inference_config=async_config,\n",
    "    initial_instance_count=1,  # number of instances\n",
    "    instance_type=\"ml.m5.4xlarge\",  # ec2 instance type\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a92d55-9592-44d0-b6fe-b8cbdf782c9d",
   "metadata": {},
   "source": [
    "Convert document text to json format and upload to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88eb3e2-7f70-41e1-aa95-7efa53f5360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find an article with well-defined background information\n",
    "result = None\n",
    "while result is None:\n",
    "    art = sample(article_names, 1)[0]\n",
    "    print(art)\n",
    "\n",
    "    with open(f\"data/raw/{art}\", \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        text = f.read().replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "        result = re.search(\"Background (.{,1000})\", text)\n",
    "\n",
    "dict = {\"inputs\": result.group(1)}  # Search for background infomation\n",
    "print(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0b51c0-8e4f-4043-8efd-f966e1452a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "async_response = predictor.predict_async(data=dict)\n",
    "\n",
    "waiter = WaiterConfig(max_attempts=24, delay=15)\n",
    "result = async_response.get_result(waiter)\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311b4a91-0422-43f5-81e6-8274bb077115",
   "metadata": {},
   "source": [
    "# 5. Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018ed5ce-5baa-4cb1-be43-02b4b9f913db",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()\n",
    "\n",
    "# Delete all S3 objects\n",
    "bucket = boto_session.resource(\"s3\").Bucket(s3_bucket)\n",
    "bucket.objects.filter(Prefix=\"sci-docs\").delete()\n",
    "os.system(f\"rm -rf data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008926ea-9fe9-455a-b6d4-ccfa1c64c013",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "a8534c14445fc6cdc3039d8140510d6736e5b4960d89f445a45d8db6afd8452b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
